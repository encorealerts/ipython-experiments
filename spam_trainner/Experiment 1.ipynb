{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "SKIP_FILES = set(['cmds'])\n",
      "\n",
      "def read_files(path):\n",
      "  for root, dir_names, file_names in os.walk(path):\n",
      "    for path in dir_names:\n",
      "      read_files(os.path.join(root, path))\n",
      "    for file_name in file_names:\n",
      "      if file_name not in SKIP_FILES:\n",
      "        file_path = os.path.join(root, file_name)\n",
      "        if os.path.isfile(file_path):\n",
      "          lines = []\n",
      "          f = open(file_path)\n",
      "          for line in f:\n",
      "            lines.append(line)\n",
      "          f.close()\n",
      "          yield file_path, lines"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create the method to load data from files\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "def build_data_frame(path, classification):\n",
      "  data_frame = pd.DataFrame({'text': [], 'class': []})\n",
      "  for file_name, lines in read_files(path):\n",
      "    for line in lines:\n",
      "      data_frame = data_frame.append(\n",
      "        pd.DataFrame({'text': [line], 'class': [classification]}))\n",
      "  data_frame.drop_duplicates(cols=['text'])\n",
      "  return data_frame"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy\n",
      "\n",
      "# Definind data source\n",
      "HAM = 0\n",
      "SPAM = 1\n",
      "\n",
      "SOURCES = [\n",
      "           ('data/spam', SPAM),\n",
      "           ('data/ham', HAM)\n",
      "           ]\n",
      "\n",
      "# Loading data\n",
      "\n",
      "data_frames = []\n",
      "for path, classification in SOURCES:\n",
      "  print path\n",
      "  new_df = build_data_frame(path, classification)\n",
      "  print len(new_df)\n",
      "  data_frames.append(new_df)\n",
      "\n",
      "data = pd.concat(data_frames)\n",
      "data['index'] = np.arange(0, len(data))\n",
      "data = data.reindex(numpy.random.permutation(data.index))\n",
      "\n",
      "print data.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "data/spam\n",
        "14321"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "data/ham\n",
        "8252"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "   class                                               text  index\n",
        "0      1  \u0000\u0000\u0000\u0001Bud1\u0000\u0000\u0010\u0000\u0000\u0000\b\u0000\u0000\u0000\u0010\u0000\u0000\u0000\u0000%\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000...      0\n",
        "0      1  #marvel #hachette #graphicnovel #theamazingspi...      1\n",
        "0      1  RT @GabbysQuilts: http://t.co/OxKXzMrMlI #circ...      2\n",
        "0      1  RT @2bproductive: 10% #Discount on #Books RARA...      3\n",
        "0      1  #vegan Cibaderm #NonGMO #hemp oil beauty produ...      4\n",
        "\n",
        "[5 rows x 3 columns]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.base import BaseEstimator\n",
      "import re\n",
      "import string\n",
      "\n",
      "class WordCounter(BaseEstimator):\n",
      "    def get_feature_names(self):\n",
      "        return np.array(['n_words', 'n_chars', 'punctuation','n_caps', 'n_hashtags', 'n_handles', 'hashtag_ratio', 'handle_ratio', \n",
      "            'caps_ratio'])\n",
      "\n",
      "    def fit(self, documents, y=None):\n",
      "        return self\n",
      "\n",
      "    def transform(self, documents):\n",
      "        count = lambda l1,l2: sum([1 for x in l1 if x in l2])\n",
      "            \n",
      "        ## some handcrafted features!\n",
      "        n_words = [len(c.split()) for c in documents]\n",
      "        n_chars = [len(c) for c in documents]\n",
      "        punctuation = [count(c, set(string.punctuation)) for c in documents]\n",
      "        # number of uppercase words\n",
      "        n_caps = [np.sum([w.isupper() for w in comment.split()])\n",
      "               for comment in documents]\n",
      "        \n",
      "        n_hashtags = [len(re.findall(r\"#(\\w+)\", c)) for c in documents]\n",
      "        n_handles = [len(re.findall(r\"@(\\w+)\", c)) for c in documents]\n",
      "        \n",
      "        hashtag_ratio = np.array(n_hashtags)/np.array(n_words)\n",
      "        handle_ratio = np.array(n_handles)/np.array(n_words)\n",
      "\n",
      "        caps_ratio = np.array(n_caps)/np.array(n_chars)\n",
      "\n",
      "        return np.array([n_words, n_chars, punctuation, n_caps,\n",
      "            n_hashtags, n_handles, hashtag_ratio, handle_ratio, caps_ratio]).T"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Testing\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.pipeline import FeatureUnion\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_selection import SelectPercentile\n",
      "from sklearn.feature_selection import chi2\n",
      "\n",
      "select = SelectPercentile(score_func=chi2, percentile=16)\n",
      "\n",
      "countvect_char = TfidfVectorizer(ngram_range=(1, 5), analyzer=\"char\", binary=False, decode_error='ignore')\n",
      "countvect_word = TfidfVectorizer(analyzer=\"word\", decode_error='ignore')\n",
      "countvect_special = WordCounter()\n",
      "\n",
      "ft1 = FeatureUnion([('chars', countvect_char), ('words', countvect_word), ('special', countvect_special)])\n",
      "ft2 = FeatureUnion([('chars', countvect_char), ('words', countvect_word)])\n",
      "\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "lr_pipeline = Pipeline([\n",
      "  ('ft',  ft1),\n",
      "  ('selection', select),\n",
      "  ('classifier',  LogisticRegression(tol=1e-8, penalty='l2', C=4)) ])\n",
      "\n",
      "#lr_pipeline.fit(numpy.asarray(data['text']), numpy.asarray(data['class']))\n",
      "\n",
      "from sklearn.svm import LinearSVC\n",
      "\n",
      "svc_pipeline = Pipeline([\n",
      "  ('ft',  ft2),\n",
      "  ('selection', select),\n",
      "  ('classifier',  LinearSVC()) ])\n",
      "\n",
      "#svc_pipeline.fit(numpy.asarray(data['text']), numpy.asarray(data['class']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Cross-validation\n",
      "from sklearn.cross_validation import KFold\n",
      "\n",
      "k_fold = KFold(n=len(data), n_folds=6, indices=False, shuffle=True)\n",
      "b_scores, svc_scores = [], []\n",
      "\n",
      "print 'Bayes score:'\n",
      "for train_indices, test_indices in k_fold:\n",
      "  train_text = numpy.asarray(data[train_indices]['text'])\n",
      "  train_y    = numpy.asarray(data[train_indices]['class'])\n",
      "\n",
      "  test_text  = numpy.asarray(data[test_indices]['text'])\n",
      "  test_y     = numpy.asarray(data[test_indices]['class'])\n",
      "\n",
      "  lr_pipeline.fit(train_text, train_y)\n",
      "  score = lr_pipeline.score(test_text, test_y)\n",
      "  b_scores.append(score)\n",
      "  pred_y = lr_pipeline.predict(test_text)\n",
      "  print metrics.confusion_matrix(test_y, pred_y)\n",
      "\n",
      "  print \n",
      "\n",
      "score = sum(b_scores) / len(b_scores)\n",
      "print score\n",
      "\n",
      "print 'Logistic Regression score:'\n",
      "for train_indices, test_indices in k_fold:\n",
      "  train_text = numpy.asarray(data[train_indices]['text'])\n",
      "  train_y    = numpy.asarray(data[train_indices]['class'])\n",
      "\n",
      "  test_text  = numpy.asarray(data[test_indices]['text'])\n",
      "  test_y     = numpy.asarray(data[test_indices]['class'])\n",
      "    \n",
      "  svc_pipeline.fit(train_text, train_y)\n",
      "  score = svc_pipeline.score(test_text, test_y)\n",
      "  svc_scores.append(score)\n",
      "  pred_y = svc_pipeline.predict(test_text)\n",
      "  print metrics.confusion_matrix(test_y, pred_y)\n",
      "\n",
      "score = sum(svc_scores) / len(svc_scores)\n",
      "print score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bayes score:\n",
        "[[1321   89]\n",
        " [  71 2282]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[[1275   54]\n",
        " [  79 2354]]"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### Confusion Matrix\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['class'], test_size=0.3, random_state=1)\n",
      "\n",
      "print \"Logistic: \"\n",
      "lr_pipeline.fit(X_train, y_train)\n",
      "pred_y = lr_pipeline.predict(X_test)\n",
      "print confusion_matrix(y_test, pred_y)\n",
      "print 'score:' + str(lr_pipeline.score(X_test, y_test))\n",
      "\n",
      "print \"SVC: \"    \n",
      "svc_pipeline.fit(X_train, y_train)\n",
      "pred_y = svc_pipeline.predict(X_test)\n",
      "print confusion_matrix(y_test, pred_y)\n",
      "print 'score:' + str(svc_pipeline.score(X_test, y_test))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Logistic: \n",
        "[[2311  154]\n",
        " [ 115 4192]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "score:0.960277613703"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "SVC: \n",
        "[[2359  106]\n",
        " [ 138 4169]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "score:0.963969285292"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Logistic: \"\n",
      "lr_pipeline.fit(numpy.asarray(data['text']), numpy.asarray(data['class']))\n",
      "\n",
      "from sklearn.externals import joblib\n",
      "joblib.dump(lr_pipeline, 'my_model.pkl', compress=9)\n",
      "\n",
      "print 'Done!!!'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Logistic: \n",
        "Done!!!"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Model predict\n",
      "\n",
      "\n",
      "\n",
      "print svc_pipeline.predict(['asfgdadsg asdgasdgdsh3rerw c43c52345v v324562dasf'])\n",
      "print len('asfgdadsg asdgasdgdsh3rerw c43c52345v v324562dasf')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0]\n",
        "49\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}