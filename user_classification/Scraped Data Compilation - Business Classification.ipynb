{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loading Scraped Data\n",
    "## Fanpage List\n",
    "This sides provides not only categories for the twitter accounts but also sub categories that will be very useful for trainning user classification tasks, but also for future topic classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>   Facebook</td>\n",
       "      <td>  facebook</td>\n",
       "      <td>   Brand</td>\n",
       "      <td> Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>    YouTube</td>\n",
       "      <td>   YouTube</td>\n",
       "      <td> Product</td>\n",
       "      <td> Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> McDonald's</td>\n",
       "      <td> McDonalds</td>\n",
       "      <td>   Brand</td>\n",
       "      <td>     Dining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>        MTV</td>\n",
       "      <td>       MTV</td>\n",
       "      <td> TV Show</td>\n",
       "      <td>        MTV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>     Disney</td>\n",
       "      <td>    Disney</td>\n",
       "      <td>   Brand</td>\n",
       "      <td>      Media</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name     handle category subcategory\n",
       "0    Facebook   facebook    Brand  Technology\n",
       "1     YouTube    YouTube  Product  Technology\n",
       "2  McDonald's  McDonalds    Brand      Dining\n",
       "3         MTV        MTV  TV Show         MTV\n",
       "4      Disney     Disney    Brand       Media"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fanpage_list = pd.read_csv('data/scraped/fanpagelist_scrapped_data_16_06_2015.csv', header=None, names=['name', 'handle','category','subcategory'])\n",
    "fanpage_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fanpage List has 2319 trained items.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Athlete</th>\n",
       "      <td> 499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organization</th>\n",
       "      <td> 459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TV Show</th>\n",
       "      <td> 274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brand</th>\n",
       "      <td> 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Musician</th>\n",
       "      <td> 202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sports Team</th>\n",
       "      <td> 183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td> 137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie</th>\n",
       "      <td>  84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Product</th>\n",
       "      <td>  60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politician</th>\n",
       "      <td>  48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Game</th>\n",
       "      <td>  37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actor</th>\n",
       "      <td>  34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>   9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Comedian</th>\n",
       "      <td>   8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reality Star</th>\n",
       "      <td>   6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TV Host</th>\n",
       "      <td>   5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Executive</th>\n",
       "      <td>   4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Character</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Author</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pro Dancer</th>\n",
       "      <td>   1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              handle\n",
       "category            \n",
       "Athlete          499\n",
       "Organization     459\n",
       "TV Show          274\n",
       "Brand            256\n",
       "Musician         202\n",
       "Sports Team      183\n",
       "News             137\n",
       "Movie             84\n",
       "Product           60\n",
       "Politician        48\n",
       "Game              37\n",
       "Actor             34\n",
       "Model              9\n",
       "Comedian           8\n",
       "Reality Star       6\n",
       "TV Host            5\n",
       "Executive          4\n",
       "Character          1\n",
       "Author             1\n",
       "Pro Dancer         1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brand_categories = ['Organization', 'TV Show', 'Brand', 'Sports Team', 'News', 'Product', 'Movie', 'Game']\n",
    "person_categories = ['Athlete', 'Musician', 'Politician', 'Actor', 'Model', 'Comedian', 'Reality Star', 'TV Host', 'Executive', 'Author', 'Pro Dancer']\n",
    "\n",
    "print('Fanpage List has', len(fanpage_list), 'trained items.')\n",
    "fanpage_list[['handle','category']].groupby('category').count().sort('handle', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###News category might be dubious\n",
    "\n",
    "The only category that is dubious is news, I'll make sure it is not filled with reporters and news outlets, but this needs manual analisys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11 </th>\n",
       "      <td> National Geographic</td>\n",
       "      <td>   NatGeo</td>\n",
       "      <td> News</td>\n",
       "      <td> Travel Channel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55 </th>\n",
       "      <td>   CNN Breaking News</td>\n",
       "      <td>   cnnbrk</td>\n",
       "      <td> News</td>\n",
       "      <td>          World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56 </th>\n",
       "      <td>                 CNN</td>\n",
       "      <td>      CNN</td>\n",
       "      <td> News</td>\n",
       "      <td>           U.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75 </th>\n",
       "      <td>                ESPN</td>\n",
       "      <td>     espn</td>\n",
       "      <td> News</td>\n",
       "      <td>         Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>      BBC World News</td>\n",
       "      <td> BBCWorld</td>\n",
       "      <td> News</td>\n",
       "      <td>          World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name    handle category     subcategory\n",
       "11   National Geographic    NatGeo     News  Travel Channel\n",
       "55     CNN Breaking News    cnnbrk     News           World\n",
       "56                   CNN       CNN     News            U.S.\n",
       "75                  ESPN      espn     News          Sports\n",
       "100       BBC World News  BBCWorld     News           World"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fanpage_list[fanpage_list['category'] == 'News'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By manual analisys, the 'News' category is filled only with News Outlets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainned brands count: 1490\n",
      "Trainned person count: 817\n"
     ]
    }
   ],
   "source": [
    "fanpage_list_brands = fanpage_list[fanpage_list['category'].isin(brand_categories)]\n",
    "fanpage_list_person = fanpage_list[fanpage_list['category'].isin(person_categories)]\n",
    "print('Trainned brands count:', len(fanpage_list_brands))\n",
    "print('Trainned person count:', len(fanpage_list_person))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Brand Index data\n",
    "A site with a collection of twitter brand handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>handle</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>                     3M</td>\n",
       "      <td>         3M</td>\n",
       "      <td> Business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>    Abbott Laboratories</td>\n",
       "      <td> abbottnews</td>\n",
       "      <td> Business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>                Actavis</td>\n",
       "      <td>    Actavis</td>\n",
       "      <td> Business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>     Advance Auto Parts</td>\n",
       "      <td>   aapdeals</td>\n",
       "      <td> Business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> Advanced Micro Devices</td>\n",
       "      <td>        amd</td>\n",
       "      <td> Business</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name      handle  category  subcategory\n",
       "0                      3M          3M  Business          NaN\n",
       "1     Abbott Laboratories  abbottnews  Business          NaN\n",
       "2                 Actavis     Actavis  Business          NaN\n",
       "3      Advance Auto Parts    aapdeals  Business          NaN\n",
       "4  Advanced Micro Devices         amd  Business          NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "social_brand_index = pd.read_csv('data/scraped/socialbrandindex_scrapped_data_19_06_2015.csv', header=None, names=['name', 'handle','category','subcategory'])\n",
    "social_brand_index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social Brand Index has 400 trained items.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td> 400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          handle\n",
       "category        \n",
       "Business     400"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Social Brand Index has', len(social_brand_index), 'trained items.')\n",
    "social_brand_index[['handle','category']].groupby('category').count().sort('handle', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainned brands count: 398\n"
     ]
    }
   ],
   "source": [
    "print('Trainned brands count:', len(social_brand_index['handle'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twibs data\n",
    "A site with a collection of twitter brand handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>   andriisedniev</td>\n",
       "      <td>   andriisedniev</td>\n",
       "      <td> Bloggers Services</td>\n",
       "      <td> Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>     danzarrella</td>\n",
       "      <td>     danzarrella</td>\n",
       "      <td> Bloggers Services</td>\n",
       "      <td> Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>   Kathy Johnson</td>\n",
       "      <td>   Kathy_Johnson</td>\n",
       "      <td> Bloggers Services</td>\n",
       "      <td> Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> Jaquoneoqhdoqhd</td>\n",
       "      <td> Jaquoneoqhdoqhd</td>\n",
       "      <td> Bloggers Services</td>\n",
       "      <td> Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>         pchaney</td>\n",
       "      <td>         pchaney</td>\n",
       "      <td> Bloggers Services</td>\n",
       "      <td> Business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            handle             name           category subcategory\n",
       "0    andriisedniev    andriisedniev  Bloggers Services    Business\n",
       "1      danzarrella      danzarrella  Bloggers Services    Business\n",
       "2    Kathy Johnson    Kathy_Johnson  Bloggers Services    Business\n",
       "3  Jaquoneoqhdoqhd  Jaquoneoqhdoqhd  Bloggers Services    Business\n",
       "4          pchaney          pchaney  Bloggers Services    Business"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twibs = pd.read_csv('data/scraped/twibs_scrapped_data_17_06_2015.csv', header=None, names=['handle', 'name', 'category', 'subcategory'])\n",
    "twibs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twibs has 2127 trained items.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bloggers Services</th>\n",
       "      <td> 279</td>\n",
       "      <td> 279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Media</th>\n",
       "      <td> 225</td>\n",
       "      <td> 225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food and Restaurants</th>\n",
       "      <td> 217</td>\n",
       "      <td> 217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Community and Education</th>\n",
       "      <td> 180</td>\n",
       "      <td> 180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td> 180</td>\n",
       "      <td> 180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health</th>\n",
       "      <td> 180</td>\n",
       "      <td> 180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Retail</th>\n",
       "      <td> 160</td>\n",
       "      <td> 160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td> 147</td>\n",
       "      <td> 147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel and Recreation</th>\n",
       "      <td> 147</td>\n",
       "      <td> 147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Web</th>\n",
       "      <td> 107</td>\n",
       "      <td> 107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Events</th>\n",
       "      <td>  80</td>\n",
       "      <td>  80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate</th>\n",
       "      <td>  60</td>\n",
       "      <td>  60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entrepreneurs</th>\n",
       "      <td>  60</td>\n",
       "      <td>  60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pop Culture</th>\n",
       "      <td>  45</td>\n",
       "      <td>  45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business Services</th>\n",
       "      <td>  20</td>\n",
       "      <td>  20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transportation</th>\n",
       "      <td>  20</td>\n",
       "      <td>  20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Insurance</th>\n",
       "      <td>  20</td>\n",
       "      <td>  20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         handle  subcategory\n",
       "category                                    \n",
       "Bloggers Services           279          279\n",
       "Media                       225          225\n",
       "Food and Restaurants        217          217\n",
       "Community and Education     180          180\n",
       "Tech                        180          180\n",
       "Health                      180          180\n",
       "Retail                      160          160\n",
       "Electronics                 147          147\n",
       "Travel and Recreation       147          147\n",
       "Web                         107          107\n",
       "Events                       80           80\n",
       "Real Estate                  60           60\n",
       "Entrepreneurs                60           60\n",
       "Pop Culture                  45           45\n",
       "Business Services            20           20\n",
       "Transportation               20           20\n",
       "Insurance                    20           20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Twibs has', len(twibs), 'trained items.')\n",
    "twibs[['handle','category','subcategory']].groupby('category').count().sort('handle', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>handle</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategory</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Music</th>\n",
       "      <td> 60</td>\n",
       "      <td> 60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gaming</th>\n",
       "      <td> 40</td>\n",
       "      <td> 40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td> 40</td>\n",
       "      <td> 40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Organizations</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Realtors</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate Investment</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radio</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Programming Languages</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Power Supply</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Platform</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pizza</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parent</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>API</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rentals</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online Retailer</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Online News Source</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News and Media</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movies</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Movie</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Misc. Hardware</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Airlines</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Magazines</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recycling</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurants</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jewelry</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>School</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Website</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Web Design</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Venues</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hotels</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coordination</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Green technology</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food and Nutrition</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hosting</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hospitals</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Healthcare</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health and Beauty</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Health Food</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Handmade Goods</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gyms</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grocery</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Destinations</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Graphic Designers</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iPhone</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Food</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fashion and Apparel</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Events</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event Planning</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Entertainment</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Electronics</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Doctors</th>\n",
       "      <td> 20</td>\n",
       "      <td> 20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Travel</th>\n",
       "      <td> 19</td>\n",
       "      <td> 19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Personalities</th>\n",
       "      <td> 17</td>\n",
       "      <td> 17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Newspapers</th>\n",
       "      <td> 16</td>\n",
       "      <td> 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>News Personality</th>\n",
       "      <td>  9</td>\n",
       "      <td>  9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Payment Gateway</th>\n",
       "      <td>  7</td>\n",
       "      <td>  7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticket sales</th>\n",
       "      <td>  7</td>\n",
       "      <td>  7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Printers</th>\n",
       "      <td>  7</td>\n",
       "      <td>  7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reality Television</th>\n",
       "      <td>  5</td>\n",
       "      <td>  5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        handle  category\n",
       "subcategory                             \n",
       "Music                       60        60\n",
       "Gaming                      40        40\n",
       "Politics                    40        40\n",
       "Organizations               20        20\n",
       "Realtors                    20        20\n",
       "Real Estate Investment      20        20\n",
       "Radio                       20        20\n",
       "Programming Languages       20        20\n",
       "Power Supply                20        20\n",
       "Platform                    20        20\n",
       "Pizza                       20        20\n",
       "Parent                      20        20\n",
       "API                         20        20\n",
       "Rentals                     20        20\n",
       "Online Retailer             20        20\n",
       "Online News Source          20        20\n",
       "News and Media              20        20\n",
       "News                        20        20\n",
       "Movies                      20        20\n",
       "Movie                       20        20\n",
       "Misc. Hardware              20        20\n",
       "Airlines                    20        20\n",
       "Magazines                   20        20\n",
       "Recycling                   20        20\n",
       "Restaurants                 20        20\n",
       "Jewelry                     20        20\n",
       "School                      20        20\n",
       "Website                     20        20\n",
       "Web Design                  20        20\n",
       "Venues                      20        20\n",
       "...                        ...       ...\n",
       "Hotels                      20        20\n",
       "Coordination                20        20\n",
       "Green technology            20        20\n",
       "Food and Nutrition          20        20\n",
       "Hosting                     20        20\n",
       "Hospitals                   20        20\n",
       "Healthcare                  20        20\n",
       "Health and Beauty           20        20\n",
       "Health Food                 20        20\n",
       "Handmade Goods              20        20\n",
       "Gyms                        20        20\n",
       "Grocery                     20        20\n",
       "Destinations                20        20\n",
       "Graphic Designers           20        20\n",
       "iPhone                      20        20\n",
       "Food                        20        20\n",
       "Fashion and Apparel         20        20\n",
       "Events                      20        20\n",
       "Event Planning              20        20\n",
       "Entertainment               20        20\n",
       "Electronics                 20        20\n",
       "Doctors                     20        20\n",
       "Travel                      19        19\n",
       "Personalities               17        17\n",
       "Newspapers                  16        16\n",
       "News Personality             9         9\n",
       "Payment Gateway              7         7\n",
       "Ticket sales                 7         7\n",
       "Printers                     7         7\n",
       "Reality Television           5         5\n",
       "\n",
       "[106 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twibs[['handle','category','subcategory']].groupby('subcategory').count().sort('handle', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most twibs data is composed by a mix of brands and persons, so it have to be classified manually and it will not be used in this current notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter to retrieve metadata\n",
    "The scraped data consist mainly of handles and categories, so we need to retrieve extra actor information for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from twitter import *\n",
    "\n",
    "token = '22911906-GR7LBJ2oil3cc27aUIAln4zur4F7CdKAKyEi6NDzi'\n",
    "token_key = 'FZbyPm1i3BMfiXKlKPuzBdRlvbenW09n8LX5OvgM85g'\n",
    "con_secret = 'cyZ6NLdySvTkhKGUGmXMKw'\n",
    "con_secret_key = '5UgOJOanohNPMVkfLY85CjzdMcNAAVBlRCyGYys'\n",
    "\n",
    "t = Twitter(auth=OAuth(token, token_key, con_secret, con_secret_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['name', 'handle', 'category', 'subcategory'], dtype='object') 398\n",
      "Index(['name', 'handle', 'category', 'subcategory'], dtype='object') 1398\n",
      "1785\n"
     ]
    }
   ],
   "source": [
    "print(social_brand_index.columns, len(social_brand_index['handle'].unique()))\n",
    "print(fanpage_list_brands.columns, len(fanpage_list_brands['handle'].unique()))\n",
    "\n",
    "brand_handles_list = pd.concat([social_brand_index['handle'], fanpage_list_brands['handle']]).unique()\n",
    "print(len(brand_handles_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "\n",
    "def export_extracted_data_to_csv(profiles, file_name):\n",
    "    keys = profiles[0].keys()\n",
    "    with open(file_name, 'w', newline='') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(profiles)\n",
    "        \n",
    "def import_dict_from_csv(file_name):\n",
    "    result = []\n",
    "    if os.path.isfile(file_name):\n",
    "        reader = csv.DictReader(open(file_name))\n",
    "\n",
    "        for row in reader:\n",
    "            result.append(row)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1649 brand actors from Twitter\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "brand_profiles = import_dict_from_csv('data/csv/extracted_twitter_actor_info.csv')\n",
    "if len(brand_profiles) == 0:\n",
    "    indices = np.arange(len(brand_handles_list))\n",
    "    max_mod = int((len(brand_handles_list)/100)+1)\n",
    "    brand_profiles = []\n",
    "    for x in range(0, max_mod):\n",
    "        handles = brand_handles_list[(indices % max_mod) == x]\n",
    "        actors = t.users.lookup(screen_name=','.join(handles), _timeout=3)\n",
    "        for actor in actors:\n",
    "            brand_profiles.append(actor)\n",
    "        print(\"Extracted\", len(actors), \"handles from twitter API\")\n",
    "\n",
    "    export_extracted_data_to_csv(brand_profiles, 'data/csv/extracted_twitter_actor_info.csv')\n",
    "\n",
    "print('Extracted', len(brand_profiles), 'brand actors from Twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in a csv file the whole downloaded profiles from Twitter API\n",
    "This data may be used further for feature selection, so it is important that we store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete profiles from csv file...\n",
      "1 . Imported tweets for 3M\n",
      "2 . Imported tweets for AmericanExpress\n",
      "3 . Imported tweets for BestBuy\n",
      "4 . Imported tweets for CBRE\n",
      "5 . Imported tweets for DanaHoldingCorp\n",
      "6 . Imported tweets for DrPepperSnapple\n",
      "7 . Imported tweets for ExelisInc\n",
      "8 . Imported tweets for Gap\n",
      "9 . Imported tweets for Kohls\n",
      "10 . Imported tweets for MarriottIntl\n",
      "11 . Imported tweets for OwensCorning\n",
      "12 . Imported tweets for PPLElectric\n",
      "13 . Imported tweets for RyderPR\n",
      "14 . Imported tweets for Staples\n",
      "15 . Imported tweets for Thrivent\n",
      "16 . Imported tweets for urscorp\n",
      "17 . Imported tweets for windowslive\n",
      "18 . Imported tweets for Xbox\n",
      "19 . Imported tweets for cnnbrk\n",
      "20 . Imported tweets for Macys\n",
      "21 . Imported tweets for gameloft\n",
      "22 . Imported tweets for London2012\n",
      "23 . Imported tweets for OldNavy\n",
      "24 . Imported tweets for kingfisherworld\n",
      "---- Persisting to CSV file\n",
      "25 . Imported tweets for ilovebeingblack\n",
      "26 . Imported tweets for redbox\n",
      "27 . Imported tweets for TheOnion\n",
      "28 . Imported tweets for ToysRUs\n",
      "29 . Imported tweets for TheSims2\n",
      "30 . Imported tweets for Roxy\n",
      "31 . Imported tweets for AirAsia\n",
      "32 . Imported tweets for Arbys\n",
      "33 . Imported tweets for NewYorker\n",
      "34 . Imported tweets for BBCR1\n",
      "35 . Imported tweets for USAirways\n",
      "36 . Imported tweets for PrincessCruises\n",
      "37 . Imported tweets for billmaher\n",
      "38 . Imported tweets for dallascowboys\n",
      "39 . Imported tweets for warriors\n",
      "40 . Imported tweets for Cardinals\n",
      "41 . Imported tweets for Suns\n",
      "42 . Imported tweets for Sixers\n",
      "43 . Imported tweets for Brewers\n",
      "44 . Imported tweets for Dbacks\n",
      "45 . Imported tweets for Senators\n",
      "46 . Imported tweets for PhilaUnion\n",
      "47 . Imported tweets for SpiderManMovie\n",
      "48 . Imported tweets for EatPrayLove\n",
      "49 . Imported tweets for UnstoppableFAN\n",
      "---- Persisting to CSV file\n",
      "50 . Imported tweets for gracecardmovie\n",
      "51 . Imported tweets for mydogtulip\n",
      "52 . Imported tweets for CW_VampDiaries\n",
      "53 . Imported tweets for TrueBloodHBO\n",
      "54 . Imported tweets for OnceABC\n",
      "55 . Imported tweets for FRINGEonFOX\n",
      "56 . Imported tweets for MadMen_AMC\n",
      "57 . Imported tweets for NBCChicagoFire\n",
      "58 . Imported tweets for PersonInterest\n",
      "59 . Imported tweets for Letterman\n",
      "60 . Imported tweets for NoOrdFamilyABC\n",
      "61 . Imported tweets for stargatecommand\n",
      "62 . Imported tweets for Medium_CBS\n",
      "63 . Imported tweets for OprahShowBTS\n",
      "64 . Imported tweets for programapanico\n",
      "65 . Imported tweets for intouchweekly\n",
      "66 . Imported tweets for TheNextWeb\n",
      "67 . Imported tweets for ColonelTribune\n",
      "68 . Imported tweets for metmuseum\n",
      "69 . Imported tweets for VisitGreecegr\n",
      "70 . Imported tweets for KUnews\n",
      "71 . Imported tweets for brooklynmuseum\n",
      "72 . Imported tweets for msstate\n",
      "73 . Imported tweets for JohnsHopkins\n",
      "74 . Imported tweets for UConn\n",
      "---- Persisting to CSV file\n",
      "75 . Imported tweets for JMU\n",
      "76 . Imported tweets for DePaulNewsroom\n",
      "77 . Imported tweets for universityofri\n",
      "78 . Imported tweets for nmsu\n",
      "79 . Imported tweets for volunteerabroad\n",
      "80 . Imported tweets for TuftsUniversity\n",
      "81 . Imported tweets for chicostate\n",
      "82 . Imported tweets for HighPointU\n",
      "83 . Imported tweets for UMaryWash\n",
      "84 . Imported tweets for colgateuniv\n",
      "85 . Imported tweets for StMarysU\n",
      "86 . Imported tweets for BaruchCollege\n",
      "87 . Imported tweets for LafCol\n",
      "88 . Imported tweets for AdelphiU\n",
      "89 . Imported tweets for Augustana_IL\n",
      "90 . Imported tweets for LaVerneAdmiss\n",
      "91 . Imported tweets for saintanselm\n",
      "92 . Imported tweets for DePauwU\n",
      "93 . Imported tweets for carrollu\n",
      "94 . Imported tweets for AbbottNews\n",
      "95 . Imported tweets for amfam\n",
      "96 . Imported tweets for ADP\n",
      "97 . Imported tweets for BigLots\n",
      "98 . Imported tweets for CBS\n",
      "99 . Imported tweets for CliffsNR\n",
      "---- Persisting to CSV file\n",
      "100 . Imported tweets for DrPeterKuDDS\n",
      "101 . Imported tweets for HenrySchein\n",
      "102 . Imported tweets for IntlPaperCo\n",
      "103 . Imported tweets for krogerco\n",
      "104 . Imported tweets for MascoCareers\n",
      "105 . Imported tweets for molinahealth\n",
      "106 . Imported tweets for nisource\n",
      "107 . Imported tweets for PACCARFinancial\n",
      "108 . Imported tweets for Safeway\n",
      "109 . Imported tweets for TC_Talks\n",
      "110 . Imported tweets for USAirways\n",
      "111 . Imported tweets for WesternSouthern\n",
      "112 . Imported tweets for BlackBerry\n",
      "113 . Imported tweets for sonyxperia\n",
      "114 . Imported tweets for CNN\n",
      "115 . Imported tweets for Avon_UK\n",
      "116 . Imported tweets for nfl\n",
      "117 . Imported tweets for tacobell\n",
      "118 . Imported tweets for TIME\n",
      "119 . Imported tweets for theBKlounge\n",
      "120 . Imported tweets for MLB\n",
      "121 . Imported tweets for TMZ\n",
      "122 . Imported tweets for Jeep\n",
      "123 . Imported tweets for AXE\n",
      "124 . Imported tweets for hpnews\n",
      "---- Persisting to CSV file\n",
      "125 . Imported tweets for BleacherReport\n",
      "126 . Imported tweets for RoyalCaribbean\n",
      "127 . Imported tweets for OldSpice\n",
      "128 . Imported tweets for WrestleMania\n",
      "129 . Imported tweets for UrbanOutfitters\n",
      "130 . Imported tweets for WWEmena\n",
      "131 . Imported tweets for ArmaniExchange\n",
      "132 . Imported tweets for WendyWilliams\n",
      "133 . Imported tweets for Patriots\n",
      "134 . Imported tweets for Broncos\n",
      "135 . Imported tweets for Colts\n",
      "136 . Imported tweets for Raptors\n",
      "137 . Imported tweets for Angels\n",
      "138 . Imported tweets for Pirates\n",
      "139 . Imported tweets for STLouisRams\n",
      "140 . Imported tweets for ToroRossoSpy\n",
      "141 . Imported tweets for FlaPanthers\n",
      "142 . Imported tweets for Avengers\n",
      "143 . Imported tweets for FasterMovie\n",
      "144 . Imported tweets for SOTWthemovie\n",
      "145 . Imported tweets for TheWarriorsWay\n",
      "146 . Imported tweets for coldweatherfilm\n",
      "147 . Imported tweets for GLEEonFOX\n",
      "148 . Imported tweets for AHSFX\n",
      "149 . Imported tweets for DancingABC\n",
      "---- Persisting to CSV file\n",
      "150 . Imported tweets for 106andpark\n",
      "151 . Imported tweets for ABCFsab\n",
      "152 . Imported tweets for ColdCase_CBS\n",
      "153 . Imported tweets for BobsBurgersFOX\n",
      "154 . Imported tweets for RaisingHopeFOX\n",
      "155 . Imported tweets for tvland\n",
      "156 . Imported tweets for FXNetworks\n",
      "157 . Imported tweets for kenburnspbs\n",
      "158 . Imported tweets for OWN_OurAmerica\n",
      "159 . Imported tweets for showdavida\n",
      "160 . Imported tweets for YahooCelebrity\n",
      "161 . Imported tweets for TheYoungTurks\n",
      "162 . Imported tweets for Star_News\n",
      "163 . Imported tweets for Yale\n",
      "164 . Imported tweets for Princeton\n",
      "165 . Imported tweets for UofA\n",
      "166 . Imported tweets for WestPointNews\n",
      "167 . Imported tweets for BostonCollege\n",
      "168 . Imported tweets for TimeOutChicago\n",
      "169 . Imported tweets for wkunews\n",
      "170 . Imported tweets for UCRiverside\n",
      "171 . Imported tweets for OralRobertsU\n",
      "172 . Imported tweets for pepperdine\n",
      "173 . Imported tweets for Portland_State\n",
      "174 . Imported tweets for FIU\n",
      "---- Persisting to CSV file\n",
      "175 . Imported tweets for UofNorthFlorida\n",
      "176 . Imported tweets for UVU\n",
      "177 . Imported tweets for NJIT\n",
      "178 . Imported tweets for stedwardsu\n",
      "179 . Imported tweets for utulsa\n",
      "180 . Imported tweets for MercerYou\n",
      "181 . Imported tweets for StLawrenceU\n",
      "182 . Imported tweets for holy_cross\n",
      "183 . Imported tweets for UMBC\n",
      "184 . Imported tweets for WidenerUniv\n",
      "185 . Imported tweets for MSMU_LA\n",
      "186 . Imported tweets for lewisandclark\n",
      "187 . Imported tweets for Actavis\n",
      "188 . Imported tweets for AIGinsurance\n",
      "189 . Imported tweets for AutoNation\n",
      "190 . Imported tweets for ClearChannel\n",
      "191 . Imported tweets for Clorox\n",
      "192 . Imported tweets for exxonmobil\n",
      "193 . Imported tweets for generalelectric\n",
      "194 . Imported tweets for hepays\n",
      "195 . Imported tweets for LimitedBrands\n",
      "196 . Imported tweets for MonsantoCo\n",
      "197 . Imported tweets for priceline\n",
      "198 . Imported tweets for SAICinc\n",
      "199 . Imported tweets for ValeroBRK\n",
      "---- Persisting to CSV file\n",
      "200 . Imported tweets for YouTube\n",
      "201 . Imported tweets for NESCAFE\n",
      "202 . Imported tweets for Target\n",
      "203 . Imported tweets for Disneyland\n",
      "204 . Imported tweets for espn\n",
      "205 . Imported tweets for Lamborghini\n",
      "206 . Imported tweets for Porsche\n",
      "207 . Imported tweets for Clinique\n",
      "208 . Imported tweets for FOXSports\n",
      "209 . Imported tweets for ComedyCentral\n",
      "210 . Imported tweets for people\n",
      "211 . Imported tweets for slurpeenation\n",
      "212 . Imported tweets for thenorthface\n",
      "213 . Imported tweets for TheGRAMMYs\n",
      "214 . Imported tweets for Dodge\n",
      "215 . Imported tweets for IGN\n",
      "216 . Imported tweets for Electrolux\n",
      "217 . Imported tweets for Fritolay\n",
      "218 . Imported tweets for Gillette\n",
      "219 . Imported tweets for BA_USA\n",
      "220 . Imported tweets for BananaRepublic\n",
      "221 . Imported tweets for PerezHilton\n",
      "222 . Imported tweets for steelers\n",
      "223 . Imported tweets for Giants\n",
      "224 . Imported tweets for ussoccer\n",
      "---- Persisting to CSV file\n",
      "225 . Imported tweets for Phillies\n",
      "226 . Imported tweets for LAGalaxy\n",
      "227 . Imported tweets for utahjazz\n",
      "228 . Imported tweets for Strikeforce\n",
      "229 . Imported tweets for SportingKC\n",
      "230 . Imported tweets for ColumbusCrewSC\n",
      "231 . Imported tweets for starwars\n",
      "232 . Imported tweets for IAmNumberFour\n",
      "233 . Imported tweets for country_strong\n",
      "234 . Imported tweets for TomShadyac\n",
      "235 . Imported tweets for 3Backyards\n",
      "236 . Imported tweets for GreysABC\n",
      "237 . Imported tweets for Lost_on_ABC\n",
      "238 . Imported tweets for NitroCircus\n",
      "239 . Imported tweets for CLEVELANDonFOX\n",
      "240 . Imported tweets for SportsNation\n",
      "241 . Imported tweets for BravoTopChef\n",
      "242 . Imported tweets for Syfy\n",
      "243 . Imported tweets for nbcfnl\n",
      "244 . Imported tweets for USA_Network\n",
      "245 . Imported tweets for NBCHarrysLaw\n",
      "246 . Imported tweets for GoodGuysonFOX\n",
      "247 . Imported tweets for Partners_CBS\n",
      "248 . Imported tweets for SuzeOrmanShow\n",
      "249 . Imported tweets for THR\n",
      "---- Persisting to CSV file\n",
      "250 . Imported tweets for RottenTomatoes\n",
      "251 . Imported tweets for BBCBreaking\n",
      "252 . Imported tweets for Stanford\n",
      "253 . Imported tweets for penn_state\n",
      "254 . Imported tweets for WestVirginiaU\n",
      "255 . Imported tweets for okstate\n",
      "256 . Imported tweets for UVA\n",
      "257 . Imported tweets for visitPA\n",
      "258 . Imported tweets for wmunews\n",
      "259 . Imported tweets for dartmouth\n",
      "260 . Imported tweets for SydneyTimeOut\n",
      "261 . Imported tweets for univofdayton\n",
      "262 . Imported tweets for usfca\n",
      "263 . Imported tweets for stonybrooku\n",
      "264 . Imported tweets for EmbryRiddle\n",
      "265 . Imported tweets for michigantech\n",
      "266 . Imported tweets for RoanokeCollege\n",
      "267 . Imported tweets for SetonHall\n",
      "268 . Imported tweets for stonehill_info\n",
      "269 . Imported tweets for MastersCollege\n",
      "270 . Imported tweets for ClarksonUniv\n",
      "271 . Imported tweets for StKate\n",
      "272 . Imported tweets for UnionUniversity\n",
      "273 . Imported tweets for SimmonsCollege\n",
      "274 . Imported tweets for OxyNews\n",
      "---- Persisting to CSV file\n",
      "275 . Imported tweets for bellarmineU\n",
      "276 . Imported tweets for AustinCollege\n",
      "277 . Imported tweets for SimpsonNews\n",
      "278 . Imported tweets for AAPDeals\n",
      "279 . Imported tweets for Ameriprise_News\n",
      "280 . Imported tweets for ConsumersEnergy\n",
      "281 . Imported tweets for DukeEnergyNews\n",
      "282 . Imported tweets for facebook\n",
      "283 . Imported tweets for LABCORP\n",
      "284 . Imported tweets for MasterCard\n",
      "285 . Imported tweets for MorganStanley\n",
      "286 . Imported tweets for nscorp\n",
      "287 . Imported tweets for ThePrincipal\n",
      "288 . Imported tweets for SallieMae\n",
      "289 . Imported tweets for StateFarm\n",
      "290 . Imported tweets for Timken\n",
      "291 . Imported tweets for McDonalds\n",
      "292 . Imported tweets for instagram\n",
      "293 . Imported tweets for hm\n",
      "294 . Imported tweets for Burberry\n",
      "295 . Imported tweets for VSPINK\n",
      "296 . Imported tweets for HBO\n",
      "297 . Imported tweets for RedeGlobo\n",
      "298 . Imported tweets for eonline\n",
      "299 . Imported tweets for VerizonWireless\n",
      "---- Persisting to CSV file\n",
      "300 . Imported tweets for IMDb\n",
      "301 . Imported tweets for TheEconomist\n",
      "302 . Imported tweets for SouthwestAir\n",
      "303 . Imported tweets for Pinterest\n",
      "304 . Imported tweets for Honda\n",
      "305 . Imported tweets for Crocs\n",
      "306 . Imported tweets for businessinsider\n",
      "307 . Imported tweets for Hooters\n",
      "308 . Imported tweets for katespadeny\n",
      "309 . Imported tweets for BreakingNews\n",
      "310 . Imported tweets for ALDO_Shoes\n",
      "311 . Imported tweets for americanapparel\n",
      "312 . Imported tweets for maddow\n",
      "313 . Imported tweets for spurs\n",
      "314 . Imported tweets for Seahawks\n",
      "315 . Imported tweets for NHLBruins\n",
      "316 . Imported tweets for Chargers\n",
      "317 . Imported tweets for Mets\n",
      "318 . Imported tweets for Royals\n",
      "319 . Imported tweets for SoundersFC\n",
      "320 . Imported tweets for ArizonaCoyotes\n",
      "321 . Imported tweets for SJEarthquakes\n",
      "322 . Imported tweets for teamtwilighters\n",
      "323 . Imported tweets for LimitlessMovie\n",
      "324 . Imported tweets for BlueVmovie\n",
      "---- Persisting to CSV file\n",
      "325 . Imported tweets for nokjthefilm\n",
      "326 . Imported tweets for TangoFilm2o11\n",
      "327 . Imported tweets for AmericanDadTBS\n",
      "328 . Imported tweets for theofficenbc\n",
      "329 . Imported tweets for RealOneTreeHill\n",
      "330 . Imported tweets for heroes\n",
      "331 . Imported tweets for TeamCoco\n",
      "332 . Imported tweets for CommunityTV\n",
      "333 . Imported tweets for SpikeTVDW\n",
      "334 . Imported tweets for SHO_Network\n",
      "335 . Imported tweets for MindyProjectFOX\n",
      "336 . Imported tweets for Malibu_Country\n",
      "337 . Imported tweets for TheNeighborsABC\n",
      "338 . Imported tweets for indecision\n",
      "339 . Imported tweets for girlsHBO\n",
      "340 . Imported tweets for NBCSports\n",
      "341 . Imported tweets for GameTrailers\n",
      "342 . Imported tweets for CNNEE\n",
      "343 . Imported tweets for VisitJamaicaNow\n",
      "344 . Imported tweets for michiganstateu\n",
      "345 . Imported tweets for UTKnoxville\n",
      "346 . Imported tweets for BrownUniversity\n",
      "347 . Imported tweets for flavorwire\n",
      "348 . Imported tweets for AmericanU\n",
      "349 . Imported tweets for MissCollege\n",
      "---- Persisting to CSV file\n",
      "350 . Imported tweets for beaSTARalliance\n",
      "351 . Imported tweets for SouthernMissNow\n",
      "352 . Imported tweets for WIUNews\n",
      "353 . Imported tweets for WeberStateU\n",
      "354 . Imported tweets for UT_Dallas\n",
      "355 . Imported tweets for murraystateuniv\n",
      "356 . Imported tweets for HofstraU\n",
      "357 . Imported tweets for oursoutheastern\n",
      "358 . Imported tweets for EmersonCollege\n",
      "359 . Imported tweets for CSUSBNews\n",
      "360 . Imported tweets for nyupoly\n",
      "361 . Imported tweets for MissouriSandT\n",
      "362 . Imported tweets for HopeCollege\n",
      "363 . Imported tweets for gustavus\n",
      "364 . Imported tweets for JohnBrownUniv\n",
      "365 . Imported tweets for WoosterEdu\n",
      "366 . Imported tweets for ohionorthern\n",
      "367 . Imported tweets for LeMoyne\n",
      "368 . Imported tweets for AMD\n",
      "369 . Imported tweets for Healthcare_ABC\n",
      "370 . Imported tweets for Avaya\n",
      "371 . Imported tweets for Boeing\n",
      "372 . Imported tweets for celanese\n",
      "373 . Imported tweets for CocaColaCo\n",
      "374 . Imported tweets for JohnDeere\n",
      "---- Persisting to CSV file\n",
      "375 . Imported tweets for DuPont_News\n",
      "376 . Imported tweets for FannieMae\n",
      "377 . Imported tweets for GM\n",
      "378 . Imported tweets for jcpenney\n",
      "379 . Imported tweets for LandOLakesKtchn\n",
      "380 . Imported tweets for MosaicCompany\n",
      "381 . Imported tweets for ParentsPressEB\n",
      "382 . Imported tweets for ProcterGamble\n",
      "383 . Imported tweets for SanDisk\n",
      "384 . Imported tweets for StateStreet\n",
      "385 . Imported tweets for TJXCo\n",
      "386 . Imported tweets for MTV\n",
      "387 . Imported tweets for NBA\n",
      "388 . Imported tweets for EASPORTSFIFA\n",
      "389 . Imported tweets for guarana\n",
      "390 . Imported tweets for PUMA\n",
      "391 . Imported tweets for Forever21\n",
      "392 . Imported tweets for DairyQueen\n",
      "393 . Imported tweets for Bershka\n",
      "394 . Imported tweets for Gatorade\n",
      "395 . Imported tweets for RE_Games\n",
      "396 . Imported tweets for theduckbrand\n",
      "397 . Imported tweets for guardian\n",
      "398 . Imported tweets for tide\n",
      "399 . Imported tweets for etnow\n",
      "---- Persisting to CSV file\n",
      "400 . Imported tweets for Nordstrom\n",
      "401 . Imported tweets for VW\n",
      "402 . Imported tweets for MaximMag\n",
      "403 . Imported tweets for Reuters\n",
      "404 . Imported tweets for MitsubishiCAN\n",
      "405 . Imported tweets for Lufthansa_USA\n",
      "406 . Imported tweets for JBLaudio\n",
      "407 . Imported tweets for FCBarcelona\n",
      "408 . Imported tweets for nyknicks\n",
      "409 . Imported tweets for LAClippers\n",
      "410 . Imported tweets for Rangers\n",
      "411 . Imported tweets for MLSinsider\n",
      "412 . Imported tweets for NHLFlyers\n",
      "413 . Imported tweets for LAKings\n",
      "414 . Imported tweets for StLouisBlues\n",
      "415 . Imported tweets for FCDallas\n",
      "416 . Imported tweets for ColoradoRapids\n",
      "417 . Imported tweets for XMenMovies\n",
      "418 . Imported tweets for BeastlyMovie\n",
      "419 . Imported tweets for lionsgatemovies\n",
      "420 . Imported tweets for TMHTthemovie\n",
      "421 . Imported tweets for NowandLaterfilm\n",
      "422 . Imported tweets for NCIS_CBS\n",
      "423 . Imported tweets for CakeBossBuddy\n",
      "424 . Imported tweets for WWENetwork\n",
      "---- Persisting to CSV file\n",
      "425 . Imported tweets for Revenge\n",
      "426 . Imported tweets for WWENXT\n",
      "427 . Imported tweets for Bourdain\n",
      "428 . Imported tweets for Nashville_ABC\n",
      "429 . Imported tweets for CovertAffairs\n",
      "430 . Imported tweets for BodyOfProofABC\n",
      "431 . Imported tweets for NBCUpAllNight\n",
      "432 . Imported tweets for GayleKing\n",
      "433 . Imported tweets for CC_Studios\n",
      "434 . Imported tweets for OKMagazine\n",
      "435 . Imported tweets for VentureBeat\n",
      "436 . Imported tweets for guardiantech\n",
      "437 . Imported tweets for lsu\n",
      "438 . Imported tweets for Univ_Of_Oregon\n",
      "439 . Imported tweets for UNC\n",
      "440 . Imported tweets for VTadmissQA\n",
      "441 . Imported tweets for UHouston\n",
      "442 . Imported tweets for UTPA\n",
      "443 . Imported tweets for BallState\n",
      "444 . Imported tweets for NIUlive\n",
      "445 . Imported tweets for TempleUniv\n",
      "446 . Imported tweets for sluconnection\n",
      "447 . Imported tweets for UHManoaNews\n",
      "448 . Imported tweets for UNHNews\n",
      "449 . Imported tweets for UofDenver\n",
      "---- Persisting to CSV file\n",
      "450 . Imported tweets for LoyolaMarymount\n",
      "451 . Imported tweets for LoyolaNOLANews\n",
      "452 . Imported tweets for LehighU\n",
      "453 . Imported tweets for georgefox\n",
      "454 . Imported tweets for butleru\n",
      "455 . Imported tweets for BryantUniv\n",
      "456 . Imported tweets for univofthesouth\n",
      "457 . Imported tweets for merrimack\n",
      "458 . Imported tweets for JohnCarrollU\n",
      "459 . Imported tweets for KenyonCollege\n",
      "460 . Imported tweets for HamlineU\n",
      "461 . Imported tweets for CovenantCollege\n",
      "462 . Imported tweets for Aetna\n",
      "463 . Imported tweets for Amgen\n",
      "464 . Imported tweets for AveryDennison\n",
      "465 . Imported tweets for BoozAllen\n",
      "466 . Imported tweets for Celgene\n",
      "467 . Imported tweets for Cognizant\n",
      "468 . Imported tweets for FedEx\n",
      "469 . Imported tweets for smuckers\n",
      "470 . Imported tweets for McDonaldsCorp\n",
      "471 . Imported tweets for MotoSolutions\n",
      "472 . Imported tweets for northropgrumman\n",
      "473 . Imported tweets for Progressive\n",
      "474 . Imported tweets for SanminaCorp\n",
      "---- Persisting to CSV file\n",
      "475 . Imported tweets for Dynamicsltd\n",
      "476 . Imported tweets for Viacom\n",
      "477 . Imported tweets for redbull\n",
      "478 . Imported tweets for AngryBirds\n",
      "479 . Imported tweets for nikesportswear\n",
      "480 . Imported tweets for snaptu\n",
      "481 . Imported tweets for tridentgum\n",
      "482 . Imported tweets for funnyordie\n",
      "483 . Imported tweets for GuitarHero\n",
      "484 . Imported tweets for Vevo\n",
      "485 . Imported tweets for Sony\n",
      "486 . Imported tweets for YahooNews\n",
      "487 . Imported tweets for HotTopic\n",
      "488 . Imported tweets for iVillage\n",
      "489 . Imported tweets for eBuddy\n",
      "490 . Imported tweets for Sears\n",
      "491 . Imported tweets for mashable\n",
      "492 . Imported tweets for TLC\n",
      "493 . Imported tweets for Avea\n",
      "494 . Imported tweets for wetseal\n",
      "495 . Imported tweets for Butterfinger\n",
      "496 . Imported tweets for hulu\n",
      "497 . Imported tweets for potterybarn\n",
      "498 . Imported tweets for realmadrid\n",
      "499 . Imported tweets for okcthunder\n",
      "---- Persisting to CSV file\n",
      "500 . Imported tweets for HoustonRockets\n",
      "501 . Imported tweets for Braves\n",
      "502 . Imported tweets for Panthers\n",
      "503 . Imported tweets for Reds\n",
      "504 . Imported tweets for Cdchivasusa\n",
      "505 . Imported tweets for mnwild\n",
      "506 . Imported tweets for NYIslanders\n",
      "507 . Imported tweets for HRTF1Team\n",
      "508 . Imported tweets for megamind\n",
      "509 . Imported tweets for SocialNetwork\n",
      "510 . Imported tweets for CharlotteBronte\n",
      "511 . Imported tweets for sassoonthemovie\n",
      "512 . Imported tweets for BurlesqueMovie\n",
      "513 . Imported tweets for Alejandr0Borges\n",
      "514 . Imported tweets for TheXFactor\n",
      "515 . Imported tweets for Mentalist_CBS\n",
      "516 . Imported tweets for spartacus_starz\n",
      "517 . Imported tweets for UglyAmericans\n",
      "518 . Imported tweets for UltimateFighter\n",
      "519 . Imported tweets for FantasyDailys\n",
      "520 . Imported tweets for mikeandmolly\n",
      "521 . Imported tweets for cspan\n",
      "522 . Imported tweets for Vegas_CBS\n",
      "523 . Imported tweets for TheMobDoctorFOX\n",
      "524 . Imported tweets for TheComedyAwards\n",
      "---- Persisting to CSV file\n",
      "525 . Imported tweets for WWEClassics\n",
      "526 . Imported tweets for THGossip\n",
      "527 . Imported tweets for radar_online\n",
      "528 . Imported tweets for nprpolitics\n",
      "529 . Imported tweets for NWF\n",
      "530 . Imported tweets for UofOklahoma\n",
      "531 . Imported tweets for univmiami\n",
      "532 . Imported tweets for LibertyU\n",
      "533 . Imported tweets for GWtweets\n",
      "534 . Imported tweets for NorthwesternU\n",
      "535 . Imported tweets for SamHoustonState\n",
      "536 . Imported tweets for uofl\n",
      "537 . Imported tweets for FloridaAtlantic\n",
      "538 . Imported tweets for GonzagaU\n",
      "539 . Imported tweets for VisitBucksPA\n",
      "540 . Imported tweets for UNLVNews\n",
      "541 . Imported tweets for azusapacific\n",
      "542 . Imported tweets for UMKansasCity\n",
      "543 . Imported tweets for BklynCollege411\n",
      "544 . Imported tweets for bentleyu\n",
      "545 . Imported tweets for univofscranton\n",
      "546 . Imported tweets for RPInews\n",
      "547 . Imported tweets for VassarNews\n",
      "548 . Imported tweets for SamfordU\n",
      "549 . Imported tweets for LoyolaMaryland\n",
      "---- Persisting to CSV file\n",
      "550 . Imported tweets for eckerdlife\n",
      "551 . Imported tweets for ConnCollege\n",
      "552 . Imported tweets for mycuboulder\n",
      "553 . Imported tweets for aflacduck\n",
      "554 . Imported tweets for andersonsgrain\n",
      "555 . Imported tweets for EFRturbo\n",
      "556 . Imported tweets for Centene\n",
      "557 . Imported tweets for FeedBackDog\n",
      "558 . Imported tweets for GoldmanSachs\n",
      "559 . Imported tweets for hpnews\n",
      "560 . Imported tweets for Designpackages\n",
      "561 . Imported tweets for MHFI\n",
      "562 . Imported tweets for Mundofootball\n",
      "563 . Imported tweets for NM_News\n",
      "564 . Imported tweets for Sealed_Air\n",
      "565 . Imported tweets for supervaluPR\n",
      "566 . Imported tweets for TATravelCenters\n",
      "567 . Imported tweets for Visa\n",
      "568 . Imported tweets for Oreo\n",
      "569 . Imported tweets for SUBWAY\n",
      "570 . Imported tweets for HISTORY\n",
      "571 . Imported tweets for ufc\n",
      "572 . Imported tweets for Zoosk\n",
      "573 . Imported tweets for AppStore\n",
      "574 . Imported tweets for dolcegabbana\n",
      "---- Persisting to CSV file\n",
      "575 . Imported tweets for BACARDI\n",
      "576 . Imported tweets for SonyElectronics\n",
      "577 . Imported tweets for Sephora\n",
      "578 . Imported tweets for adidasrunning\n",
      "579 . Imported tweets for CocaCola\n",
      "580 . Imported tweets for Reebok\n",
      "581 . Imported tweets for bebe_Stores\n",
      "582 . Imported tweets for TOMS\n",
      "583 . Imported tweets for YahooSports\n",
      "584 . Imported tweets for BigPrize\n",
      "585 . Imported tweets for LiveNation\n",
      "586 . Imported tweets for AMC_TV\n",
      "587 . Imported tweets for dkny\n",
      "588 . Imported tweets for MarthaStewart\n",
      "589 . Imported tweets for ChelseaFC\n",
      "590 . Imported tweets for BesiktASK\n",
      "591 . Imported tweets for McLarenF1\n",
      "592 . Imported tweets for trailblazers\n",
      "593 . Imported tweets for whitesox\n",
      "594 . Imported tweets for AZCardinals\n",
      "595 . Imported tweets for SanJoseSharks\n",
      "596 . Imported tweets for Nationals\n",
      "597 . Imported tweets for BlueJacketsNHL\n",
      "598 . Imported tweets for WpgThrashers\n",
      "599 . Imported tweets for ItsMeBuddyThElf\n",
      "---- Persisting to CSV file\n",
      "600 . Imported tweets for OfficialATeam\n",
      "601 . Imported tweets for backupplanmovie\n",
      "602 . Imported tweets for BCNYthemovie\n",
      "603 . Imported tweets for HomerJSimpson\n",
      "604 . Imported tweets for CSIMiami_CBS\n",
      "605 . Imported tweets for SonsofAnarchy\n",
      "606 . Imported tweets for DrOz\n",
      "607 . Imported tweets for NewGirlonFOX\n",
      "608 . Imported tweets for WWEStudios\n",
      "609 . Imported tweets for AmazingRace_CBS\n",
      "610 . Imported tweets for TheCloser_TNT\n",
      "611 . Imported tweets for TributeToTroops\n",
      "612 . Imported tweets for LastManABC\n",
      "613 . Imported tweets for NBCTheNewNormal\n",
      "614 . Imported tweets for TheFlashForward\n",
      "615 . Imported tweets for RunningWildeFOX\n",
      "616 . Imported tweets for Rally4Sanity\n",
      "617 . Imported tweets for CNBC\n",
      "618 . Imported tweets for AP\n",
      "619 . Imported tweets for guardiannews\n",
      "620 . Imported tweets for UMich\n",
      "621 . Imported tweets for good\n",
      "622 . Imported tweets for IUBloomington\n",
      "623 . Imported tweets for UNTnews\n",
      "624 . Imported tweets for UofUNews\n",
      "---- Persisting to CSV file\n",
      "625 . Imported tweets for GeorgiaTech\n",
      "626 . Imported tweets for TCU\n",
      "627 . Imported tweets for missouristate\n",
      "628 . Imported tweets for ODUnow\n",
      "629 . Imported tweets for bgsu\n",
      "630 . Imported tweets for UNCG\n",
      "631 . Imported tweets for UofR\n",
      "632 . Imported tweets for uofsandiego\n",
      "633 . Imported tweets for montclairstateu\n",
      "634 . Imported tweets for uakron\n",
      "635 . Imported tweets for cwru\n",
      "636 . Imported tweets for Sweetbriaredu\n",
      "637 . Imported tweets for tayloru\n",
      "638 . Imported tweets for Trinity_U\n",
      "639 . Imported tweets for GOSHENCOLLEGE\n",
      "640 . Imported tweets for Macalester\n",
      "641 . Imported tweets for BARDCollege\n",
      "642 . Imported tweets for FandMCollege\n",
      "643 . Imported tweets for juniatacollege\n",
      "644 . Imported tweets for alfredu\n",
      "645 . Imported tweets for AGCOcorp\n",
      "646 . Imported tweets for Anixter\n",
      "647 . Imported tweets for Avnet\n",
      "648 . Imported tweets for bostonsci\n",
      "649 . Imported tweets for energyinsights\n",
      "---- Persisting to CSV file\n",
      "650 . Imported tweets for XFINITY\n",
      "651 . Imported tweets for ReikiRosarioFC\n",
      "652 . Imported tweets for GoodyearBlimp\n",
      "653 . Imported tweets for HillshireBrands\n",
      "654 . Imported tweets for McKesson\n",
      "655 . Imported tweets for mutualofomaha\n",
      "656 . Imported tweets for Publix\n",
      "657 . Imported tweets for symantec\n",
      "658 . Imported tweets for Visteon\n",
      "659 . Imported tweets for SamsungMobile\n",
      "660 . Imported tweets for VictoriasSecret\n",
      "661 . Imported tweets for GarethH\n",
      "662 . Imported tweets for Playboy\n",
      "663 . Imported tweets for nokia\n",
      "664 . Imported tweets for frappuccino\n",
      "665 . Imported tweets for TommyHilfiger\n",
      "666 . Imported tweets for benandjerrys\n",
      "667 . Imported tweets for pandora_radio\n",
      "668 . Imported tweets for NintendoAmerica\n",
      "669 . Imported tweets for MSN\n",
      "670 . Imported tweets for billabong1973\n",
      "671 . Imported tweets for SwatchUS\n",
      "672 . Imported tweets for UnderArmour\n",
      "673 . Imported tweets for CarnivalCruise\n",
      "674 . Imported tweets for TAMAirlines\n",
      "---- Persisting to CSV file\n",
      "675 . Imported tweets for Wimbledon\n",
      "676 . Imported tweets for WWEpublishing\n",
      "677 . Imported tweets for ColdStone\n",
      "678 . Imported tweets for TechCrunch\n",
      "679 . Imported tweets for ThisIsGMC\n",
      "680 . Imported tweets for Arsenal\n",
      "681 . Imported tweets for DFB_Team\n",
      "682 . Imported tweets for Pacers\n",
      "683 . Imported tweets for MiamiDolphins\n",
      "684 . Imported tweets for memgrizz\n",
      "685 . Imported tweets for WashWizards\n",
      "686 . Imported tweets for TBBuccaneers\n",
      "687 . Imported tweets for Marlins\n",
      "688 . Imported tweets for TimbersFC\n",
      "689 . Imported tweets for nationalteambg\n",
      "690 . Imported tweets for HTTYDragon\n",
      "691 . Imported tweets for Super8Movie\n",
      "692 . Imported tweets for GulliversMovie\n",
      "693 . Imported tweets for LiberalArtMovie\n",
      "694 . Imported tweets for MrBean\n",
      "695 . Imported tweets for GameOfThrones\n",
      "696 . Imported tweets for ModernFam\n",
      "697 . Imported tweets for SHO_weeds\n",
      "698 . Imported tweets for nbc30rock\n",
      "699 . Imported tweets for Dallas_TNT\n",
      "---- Persisting to CSV file\n",
      "700 . Imported tweets for andersoncooper\n",
      "701 . Imported tweets for BlueBloods_CBS\n",
      "702 . Imported tweets for DefianceWorld\n",
      "703 . Imported tweets for FairlyLegal\n",
      "704 . Imported tweets for BenandKateFOX\n",
      "705 . Imported tweets for ccinsider\n",
      "706 . Imported tweets for CityVille\n",
      "707 . Imported tweets for Gawker\n",
      "708 . Imported tweets for TVGuide\n",
      "709 . Imported tweets for SAI\n",
      "710 . Imported tweets for wef\n",
      "711 . Imported tweets for AuburnU\n",
      "712 . Imported tweets for UCF\n",
      "713 . Imported tweets for Baylor\n",
      "714 . Imported tweets for NCState\n",
      "715 . Imported tweets for txst\n",
      "716 . Imported tweets for Northeastern\n",
      "717 . Imported tweets for EmoryUniversity\n",
      "718 . Imported tweets for UCSDnews\n",
      "719 . Imported tweets for GVSU\n",
      "720 . Imported tweets for NAU\n",
      "721 . Imported tweets for VillanovaU\n",
      "722 . Imported tweets for DrakeUniversity\n",
      "723 . Imported tweets for BelmontUniv\n",
      "724 . Imported tweets for coschoolofmines\n",
      "---- Persisting to CSV file\n",
      "725 . Imported tweets for VisitIndiana\n",
      "726 . Imported tweets for RICNews\n",
      "727 . Imported tweets for CalvinCollege\n",
      "728 . Imported tweets for stmarysca\n",
      "729 . Imported tweets for FUAdmission\n",
      "730 . Imported tweets for FairfieldU\n",
      "731 . Imported tweets for SkidmoreCollege\n",
      "732 . Imported tweets for whitworth\n",
      "733 . Imported tweets for woffordcollege\n",
      "734 . Imported tweets for RhodesCollege\n",
      "735 . Imported tweets for ApacheCorp\n",
      "736 . Imported tweets for bmsnews\n",
      "737 . Imported tweets for CenturyLink\n",
      "738 . Imported tweets for ConAgraFoods\n",
      "739 . Imported tweets for DICKS\n",
      "740 . Imported tweets for Ecolab\n",
      "741 . Imported tweets for FifthThird\n",
      "742 . Imported tweets for HomeDepot\n",
      "743 . Imported tweets for LibertyMutual\n",
      "744 . Imported tweets for MWVPackaging\n",
      "745 . Imported tweets for MylanNews\n",
      "746 . Imported tweets for PepsiCo\n",
      "747 . Imported tweets for SempraEnergy\n",
      "748 . Imported tweets for TysonFoods\n",
      "749 . Imported tweets for VooDooZipLine\n",
      "---- Persisting to CSV file\n",
      "750 . Imported tweets for kfc\n",
      "751 . Imported tweets for amazon\n",
      "752 . Imported tweets for adidassoccer\n",
      "753 . Imported tweets for ScuderiaFerrari\n",
      "754 . Imported tweets for LACOSTE\n",
      "755 . Imported tweets for Kohls\n",
      "756 . Imported tweets for nytimes\n",
      "757 . Imported tweets for BaskinRobbins\n",
      "758 . Imported tweets for Gap\n",
      "759 . Imported tweets for msnbc\n",
      "760 . Imported tweets for Expedia\n",
      "761 . Imported tweets for LOCCITANE\n",
      "762 . Imported tweets for Chilis\n",
      "763 . Imported tweets for HardRock\n",
      "764 . Imported tweets for USATODAY\n",
      "765 . Imported tweets for stridegum\n",
      "766 . Imported tweets for BoseService\n",
      "767 . Imported tweets for SInow\n",
      "768 . Imported tweets for Logitech\n",
      "769 . Imported tweets for FOXTV\n",
      "770 . Imported tweets for AustralianOpen\n",
      "771 . Imported tweets for LFC\n",
      "772 . Imported tweets for RedSox\n",
      "773 . Imported tweets for RAIDERS\n",
      "774 . Imported tweets for Cut4\n",
      "---- Persisting to CSV file\n",
      "775 . Imported tweets for CanadiensMTL\n",
      "776 . Imported tweets for Browns\n",
      "777 . Imported tweets for Jaguars\n",
      "778 . Imported tweets for NHLCanes\n",
      "779 . Imported tweets for ManorF1Team\n",
      "780 . Imported tweets for blindsidemovie\n",
      "781 . Imported tweets for GreenHornet\n",
      "782 . Imported tweets for AskMirrorMirror\n",
      "783 . Imported tweets for vanishingon7th\n",
      "784 . Imported tweets for SpongeBob\n",
      "785 . Imported tweets for CrimMinds_CBS\n",
      "786 . Imported tweets for CSINY_CBS\n",
      "787 . Imported tweets for FallonTonight\n",
      "788 . Imported tweets for TheXFactorUSA\n",
      "789 . Imported tweets for HBOboxing\n",
      "790 . Imported tweets for TMNTMaster\n",
      "791 . Imported tweets for klgandhoda\n",
      "792 . Imported tweets for hotnclevelandtv\n",
      "793 . Imported tweets for Nightline\n",
      "794 . Imported tweets for Boss_Starz\n",
      "795 . Imported tweets for AnimalPractice\n",
      "796 . Imported tweets for OWN_DeliverMe\n",
      "797 . Imported tweets for MafiaWars\n",
      "798 . Imported tweets for HuffPostCeleb\n",
      "799 . Imported tweets for SocialTimes\n",
      "---- Persisting to CSV file\n",
      "800 . Imported tweets for NBCNewsTravel\n",
      "801 . Imported tweets for MIT\n",
      "802 . Imported tweets for ExploreCanada\n",
      "803 . Imported tweets for azcentral\n",
      "804 . Imported tweets for UMNews\n",
      "805 . Imported tweets for UofSC\n",
      "806 . Imported tweets for HowardU\n",
      "807 . Imported tweets for GeorgiaSouthern\n",
      "808 . Imported tweets for MiamiUAdmission\n",
      "809 . Imported tweets for williamandmary\n",
      "810 . Imported tweets for binghamtonu\n",
      "811 . Imported tweets for TulaneNews\n",
      "812 . Imported tweets for RiceUniversity\n",
      "813 . Imported tweets for seattleu\n",
      "814 . Imported tweets for elonuniversity\n",
      "815 . Imported tweets for websteru\n",
      "816 . Imported tweets for SUNYGeneseo\n",
      "817 . Imported tweets for nkuedu\n",
      "818 . Imported tweets for urichmond\n",
      "819 . Imported tweets for SeattlePacific\n",
      "820 . Imported tweets for RMU\n",
      "821 . Imported tweets for WFNewsCenter\n",
      "822 . Imported tweets for OhioWesleyan\n",
      "823 . Imported tweets for northcentralcol\n",
      "824 . Imported tweets for pomonacollege\n",
      "---- Persisting to CSV file\n",
      "825 . Imported tweets for WartburgCollege\n",
      "826 . Imported tweets for airproducts\n",
      "827 . Imported tweets for ApolloCardiff\n",
      "828 . Imported tweets for BHInc\n",
      "829 . Imported tweets for Broadcom\n",
      "830 . Imported tweets for conocophillips\n",
      "831 . Imported tweets for MCRagency\n",
      "832 . Imported tweets for SCE\n",
      "833 . Imported tweets for FirstData\n",
      "834 . Imported tweets for Graybar\n",
      "835 . Imported tweets for JetBlue\n",
      "836 . Imported tweets for lincolnfingroup\n",
      "837 . Imported tweets for nashfinch\n",
      "838 . Imported tweets for oreillyauto\n",
      "839 . Imported tweets for kiewit\n",
      "840 . Imported tweets for QuestDX\n",
      "841 . Imported tweets for SherwinWilliams\n",
      "842 . Imported tweets for WRBerkleyCorp\n",
      "843 . Imported tweets for Converse\n",
      "844 . Imported tweets for NickelodeonTV\n",
      "845 . Imported tweets for Doritos\n",
      "846 . Imported tweets for tatadocomo\n",
      "847 . Imported tweets for BBCWorld\n",
      "848 . Imported tweets for MACcosmetics\n",
      "849 . Imported tweets for eBay\n",
      "---- Persisting to CSV file\n",
      "850 . Imported tweets for Photoshop\n",
      "851 . Imported tweets for VH1\n",
      "852 . Imported tweets for TMobile\n",
      "853 . Imported tweets for nprnews\n",
      "854 . Imported tweets for Quiksilver\n",
      "855 . Imported tweets for swarovski\n",
      "856 . Imported tweets for 7eleven\n",
      "857 . Imported tweets for CatchAChoo\n",
      "858 . Imported tweets for AskDotCom\n",
      "859 . Imported tweets for PetSmart\n",
      "860 . Imported tweets for CNET\n",
      "861 . Imported tweets for nbc\n",
      "862 . Imported tweets for deadspace\n",
      "863 . Imported tweets for acmilan\n",
      "864 . Imported tweets for packers\n",
      "865 . Imported tweets for Eagles\n",
      "866 . Imported tweets for DetroitRedWings\n",
      "867 . Imported tweets for NYRangers\n",
      "868 . Imported tweets for BlueJays\n",
      "869 . Imported tweets for Rockies\n",
      "870 . Imported tweets for SauberF1Team\n",
      "871 . Imported tweets for HoustonDynamo\n",
      "872 . Imported tweets for FCBarcelona_es\n",
      "873 . Imported tweets for BigMommasMovie\n",
      "874 . Imported tweets for TheMechanic\n",
      "---- Persisting to CSV file\n",
      "875 . Imported tweets for RestrepoMovie\n",
      "876 . Imported tweets for AlmegaProjects\n",
      "877 . Imported tweets for FamilyGuyonFOX\n",
      "878 . Imported tweets for BBC_TopGear\n",
      "879 . Imported tweets for KUWTK\n",
      "880 . Imported tweets for TheDailyShow\n",
      "881 . Imported tweets for PrivatePractice\n",
      "882 . Imported tweets for PBS\n",
      "883 . Imported tweets for CW_network\n",
      "884 . Imported tweets for ABCSharkTank\n",
      "885 . Imported tweets for SuburgatoryABC\n",
      "886 . Imported tweets for IFC\n",
      "887 . Imported tweets for NBCGo_On\n",
      "888 . Imported tweets for TheDefendersCBS\n",
      "889 . Imported tweets for AnnaAndKristina\n",
      "890 . Imported tweets for PlantsvsZombies\n",
      "891 . Imported tweets for gamespot\n",
      "892 . Imported tweets for seattletimes\n",
      "893 . Imported tweets for usatodaytravel\n",
      "894 . Imported tweets for OhioState\n",
      "895 . Imported tweets for Cal\n",
      "896 . Imported tweets for ASU\n",
      "897 . Imported tweets for UChicago\n",
      "898 . Imported tweets for OahuVB\n",
      "899 . Imported tweets for CofOHardWorkU\n",
      "---- Persisting to CSV file\n",
      "900 . Imported tweets for SIUC\n",
      "901 . Imported tweets for RutgersU\n",
      "902 . Imported tweets for MTSUNews\n",
      "903 . Imported tweets for IllinoisStateU\n",
      "904 . Imported tweets for wwunews\n",
      "905 . Imported tweets for kennesawstate\n",
      "906 . Imported tweets for UofTampa\n",
      "907 . Imported tweets for berrycollege\n",
      "908 . Imported tweets for UOPacific\n",
      "909 . Imported tweets for BucknellU\n",
      "910 . Imported tweets for BarnardCollege\n",
      "911 . Imported tweets for BowdoinCollege\n",
      "912 . Imported tweets for DenisonU\n",
      "913 . Imported tweets for SCSTATE1896\n",
      "914 . Imported tweets for UnionCollegeNY\n",
      "915 . Imported tweets for TuskegeeUniv\n",
      "916 . Imported tweets for BethelU\n",
      "917 . Imported tweets for WestmontNews\n",
      "918 . Imported tweets for dordtcollege\n",
      "919 . Imported tweets for Alcoa\n",
      "920 . Imported tweets for Apple\n",
      "921 . Imported tweets for CAinc\n",
      "922 . Imported tweets for DillardsStores\n",
      "923 . Imported tweets for FluorCorp\n",
      "924 . Imported tweets for Group1Auto\n",
      "---- Persisting to CSV file\n",
      "925 . Imported tweets for HormelFoodsCorp\n",
      "926 . Imported tweets for JNJNews\n",
      "927 . Imported tweets for LiveNation\n",
      "928 . Imported tweets for Merck\n",
      "929 . Imported tweets for Nationwide\n",
      "930 . Imported tweets for officedepot\n",
      "931 . Imported tweets for PetSmart\n",
      "932 . Imported tweets for RalphLauren\n",
      "933 . Imported tweets for SimonPropertyGp\n",
      "934 . Imported tweets for UGI_Utilities\n",
      "935 . Imported tweets for grainger\n",
      "936 . Imported tweets for PlayStation\n",
      "937 . Imported tweets for pizzahut\n",
      "938 . Imported tweets for Marvel\n",
      "939 . Imported tweets for VANS_66\n",
      "940 . Imported tweets for TheSims\n",
      "941 . Imported tweets for LEGO_Group\n",
      "942 . Imported tweets for KLM\n",
      "943 . Imported tweets for nikebasketball\n",
      "944 . Imported tweets for JaguarUKPR\n",
      "945 . Imported tweets for VEJA\n",
      "946 . Imported tweets for HuffingtonPost\n",
      "947 . Imported tweets for ExpressLife\n",
      "948 . Imported tweets for ABCFamily\n",
      "949 . Imported tweets for Snapple\n",
      "---- Persisting to CSV file\n",
      "950 . Imported tweets for Diablo\n",
      "951 . Imported tweets for PapaJohns\n",
      "952 . Imported tweets for Timberland\n",
      "953 . Imported tweets for tntdrama\n",
      "954 . Imported tweets for thehomedepot\n",
      "955 . Imported tweets for WholeFoods\n",
      "956 . Imported tweets for DisneyStore\n",
      "957 . Imported tweets for Lakers\n",
      "958 . Imported tweets for Chivas\n",
      "959 . Imported tweets for SFGiants\n",
      "960 . Imported tweets for HoustonTexans\n",
      "961 . Imported tweets for MNTimberwolves\n",
      "962 . Imported tweets for Twins\n",
      "963 . Imported tweets for astros\n",
      "964 . Imported tweets for TBLightning\n",
      "965 . Imported tweets for ChicagoFire\n",
      "966 . Imported tweets for WilliamsRacing\n",
      "967 . Imported tweets for SingleMomsClub\n",
      "968 . Imported tweets for TotalRecall\n",
      "969 . Imported tweets for hopesprings\n",
      "970 . Imported tweets for BudClaymanOC87\n",
      "971 . Imported tweets for SouthPark\n",
      "972 . Imported tweets for SHO_Dexter\n",
      "973 . Imported tweets for CSI_CBS\n",
      "974 . Imported tweets for ghostwhisperer\n",
      "---- Persisting to CSV file\n",
      "975 . Imported tweets for Survivor_Tweet\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "977 . Imported tweets for fvcorneliamarie\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "979 . Imported tweets for TOUCHonTV\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "981 . Imported tweets for NBCWhitney\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "983 . Imported tweets for NYC22_CBS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "985 . Imported tweets for Variety\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "987 . Imported tweets for NikkiFinke\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "989 . Imported tweets for BYU\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "992 . Imported tweets for UMRELATIONS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "994 . Imported tweets for IowaStateUNews\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "996 . Imported tweets for SMU\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "998 . Imported tweets for UCOBronchos\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "---- Persisting to CSV file\n",
      "1000 . Imported tweets for UWM\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1002 . Imported tweets for smithcollege\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1004 . Imported tweets for PLUNEWS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1006 . Imported tweets for HamiltonCollege\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1008 . Imported tweets for CalLutheran\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1010 . Imported tweets for AmherstCollege\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1012 . Imported tweets for WabashCollege\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1014 . Imported tweets for Applied_Blog\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1016 . Imported tweets for ch2mhill\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1018 . Imported tweets for FMC_Tech\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1020 . Imported tweets for LockheedMartin\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1022 . Imported tweets for pfizer_news\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1024 . Imported tweets for UnionPacific\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1026 . Imported tweets for WWE\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1028 . Imported tweets for drpepper\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1030 . Imported tweets for FoxNews\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1032 . Imported tweets for TiffanyAndCo\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1034 . Imported tweets for CAPRICHO\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1036 . Imported tweets for Versace\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1038 . Imported tweets for PandaExpress\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1040 . Imported tweets for tjmaxx\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1042 . Imported tweets for WTA\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1044 . Imported tweets for SEGA\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1046 . Imported tweets for redbullf1spy\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1048 . Imported tweets for penguins\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "---- Persisting to CSV file\n",
      "1050 . Imported tweets for Lotus_F1Team\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1052 . Imported tweets for BuffaloSabres\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1054 . Imported tweets for ForceIndiaF1\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1056 . Imported tweets for CowboysAliens\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1058 . Imported tweets for arbitragemovie\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1060 . Imported tweets for ABCFpll\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1062 . Imported tweets for nbcsnl\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1064 . Imported tweets for HawaiiFive0CBS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1066 . Imported tweets for 60Minutes\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1068 . Imported tweets for HBODocs\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1070 . Imported tweets for MyDadSays_CBS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1072 . Imported tweets for TheBeatlesRB\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1074 . Imported tweets for baltimoresun\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1076 . Imported tweets for UTAustin\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1078 . Imported tweets for TexasTech\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1080 . Imported tweets for UNLNews\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1082 . Imported tweets for XavierUniv\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1084 . Imported tweets for uofmemphis\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1086 . Imported tweets for UBNewsSource\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1088 . Imported tweets for SFBART\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1090 . Imported tweets for ChapmanU\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1092 . Imported tweets for TrumanState\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1094 . Imported tweets for DickinsonCol\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1096 . Imported tweets for WesternColoU\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1098 . Imported tweets for MillikinU\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "---- Persisting to CSV file\n",
      "1100 . Imported tweets for GC_CampusNews\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1102 . Imported tweets for TradeADMIS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1104 . Imported tweets for backdropsource\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1106 . Imported tweets for Corning\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1108 . Imported tweets for footlocker\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1110 . Imported tweets for HIIndustries\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1112 . Imported tweets for Lowes\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1114 . Imported tweets for NetApp\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1117 . Imported tweets for askRegions\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1119 . Imported tweets for TDSCorporate\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1121 . Imported tweets for Starbucks\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1123 . Imported tweets for adidasUS\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "---- Persisting to CSV file\n",
      "1125 . Imported tweets for DunkinDonuts\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1127 . Imported tweets for Audi\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1129 . Imported tweets for CokeZero\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1131 . Imported tweets for sonic_hedgehog\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1133 . Imported tweets for vitaminwater\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1135 . Imported tweets for EW\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1137 . Imported tweets for DSWShoeLovers\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1139 . Imported tweets for tonyhsieh\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1141 . Imported tweets for spike\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1143 . Imported tweets for NASCAR\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1145 . Imported tweets for nyjets\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1147 . Imported tweets for Bengals\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1149 . Imported tweets for NHLDevils\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1151 . Imported tweets for FastFurious\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1153 . Imported tweets for Loopermovie\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1155 . Imported tweets for tambienlalluvia\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1157 . Imported tweets for NBCTheVoice\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1159 . Imported tweets for TODAYshow\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1161 . Imported tweets for nbcchuck\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1163 . Imported tweets for CBSTweet\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1165 . Imported tweets for DOLLHOUSEonFOX\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1167 . Imported tweets for MyGenerationABC\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1169 . Imported tweets for GearsofWar\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1171 . Imported tweets for TheWrap\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1173 . Imported tweets for Colorado\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "---- Persisting to CSV file\n",
      "1175 . Imported tweets for universityofga\n",
      "1176 . Importing tweets for oregonstateuniv\n",
      "1177 . Imported tweets for Georgetown\n",
      "1178 . Importing tweets for uiowa\n",
      "1179 . Imported tweets for WSUPullman\n",
      "1180 . Importing tweets for CMUniversity\n",
      "1181 . Imported tweets for PittTweet\n",
      "1182 . Importing tweets for youngstownstate\n",
      "1183 . Imported tweets for UICnews\n",
      "1184 . Importing tweets for IUPUI\n",
      "1185 . Imported tweets for uvmvermont\n",
      "1186 . Importing tweets for IthacaCollege\n",
      "1187 . Imported tweets for emichedu\n",
      "1188 . Importing tweets for colbysawyer\n",
      "1189 . Imported tweets for SpelmanCollege\n",
      "1190 . Importing tweets for TCNJ\n",
      "1191 . Imported tweets for ColbyCollege\n",
      "1192 . Importing tweets for BatesCollege\n",
      "1193 . Imported tweets for ClarkUniversity\n",
      "1194 . Importing tweets for MeredithCollege\n",
      "1195 . Imported tweets for pacificu\n",
      "1196 . Importing tweets for stnorbert\n",
      "1197 . Imported tweets for sunyesf\n",
      "1198 . Importing tweets for AllyBank\n",
      "1199 . Imported tweets for ArrowGlobal\n",
      "---- Persisting to CSV file\n",
      "1200 . Importing tweets for BNBuzz\n",
      "1201 . Imported tweets for CampbellSoupCo\n",
      "1202 . Importing tweets for EmpowerNetCorp\n",
      "1203 . Imported tweets for Ford\n",
      "1204 . Importing tweets for harleydavidson\n",
      "1205 . Imported tweets for ITWDynatec\n",
      "1206 . Importing tweets for VegasConcierge\n",
      "1207 . Imported tweets for NewYorkLife\n",
      "1208 . Importing tweets for OmniGroup\n",
      "1209 . Imported tweets for RepublicService\n",
      "1210 . Importing tweets for SouthernCompany\n",
      "1211 . Imported tweets for Disney\n",
      "1212 . Importing tweets for Discovery\n",
      "1213 . Imported tweets for adidasoriginals\n",
      "1214 . Importing tweets for firefox\n",
      "1215 . Imported tweets for gucci\n",
      "1216 . Importing tweets for skol\n",
      "1217 . Imported tweets for AEO\n",
      "1218 . Importing tweets for Mango\n",
      "1219 . Imported tweets for Nestle\n",
      "1220 . Importing tweets for ABC\n",
      "1221 . Imported tweets for Coach\n",
      "1222 . Importing tweets for Cheesecake\n",
      "1223 . Imported tweets for Walgreens\n",
      "1224 . Importing tweets for chevrolet\n",
      "---- Persisting to CSV file\n",
      "1225 . Imported tweets for goodhealth\n",
      "1226 . Importing tweets for Outback\n",
      "1227 . Imported tweets for OWNTV\n",
      "1228 . Importing tweets for juicycouture\n",
      "1229 . Imported tweets for TripAdvisor\n",
      "1230 . Importing tweets for Uber\n",
      "1231 . Imported tweets for edhardy\n",
      "1232 . Importing tweets for NYMag\n",
      "1233 . Imported tweets for MiamiHEAT\n",
      "1234 . Importing tweets for dallasmavs\n",
      "1235 . Imported tweets for OrlandoMagic\n",
      "1236 . Importing tweets for Vikings\n",
      "1237 . Imported tweets for PelicansNBA\n",
      "1238 . Importing tweets for Indians\n",
      "1239 . Imported tweets for buffalobills\n",
      "1240 . Importing tweets for EdmontonOilers\n",
      "1241 . Imported tweets for RealSaltLake\n",
      "1242 . Importing tweets for Twilight\n",
      "1243 . Imported tweets for ResidentEvil\n",
      "1244 . Importing tweets for OddLifeMovie\n",
      "1245 . Imported tweets for Sanctum_3D\n",
      "1246 . Importing tweets for bestworstmovie\n",
      "1247 . Imported tweets for BigBang_CBS\n",
      "1248 . Importing tweets for gossipgirl\n",
      "1249 . Imported tweets for MTVLA\n",
      "---- Persisting to CSV file\n",
      "1250 . Importing tweets for GMA\n",
      "1251 . Imported tweets for nbcagt\n",
      "1252 . Importing tweets for 2BrokeGirls\n",
      "1253 . Imported tweets for BrosAndSis_ABC\n",
      "1254 . Importing tweets for sesamestreet\n",
      "1255 . Imported tweets for ccstandup\n",
      "1256 . Importing tweets for 666ParkAve_ABC\n",
      "1257 . Imported tweets for PiersMorganLive\n",
      "1258 . Importing tweets for legendofneil\n",
      "1259 . Imported tweets for LoneStaronFOX\n",
      "1260 . Importing tweets for popcap\n",
      "1261 . Imported tweets for Newsweek\n",
      "1262 . Importing tweets for thesuperficial\n",
      "1263 . Imported tweets for NASA\n",
      "1264 . Importing tweets for TAMU\n",
      "1265 . Imported tweets for DukeNews\n",
      "1266 . Importing tweets for Caltech\n",
      "1267 . Imported tweets for Penn\n",
      "1268 . Importing tweets for UofMaryland\n",
      "1269 . Imported tweets for ucdavis\n",
      "1270 . Imported tweets for UMassAmherst\n",
      "1271 . Importing tweets for TheNewSchool\n",
      "1272 . Imported tweets for USUAggies\n",
      "1273 . Importing tweets for SJSU\n",
      "1274 . Imported tweets for calpoly\n",
      "---- Persisting to CSV file\n",
      "1275 . Importing tweets for SFSU\n",
      "1276 . Imported tweets for EastCarolina\n",
      "1277 . Importing tweets for Morehouse\n",
      "1278 . Imported tweets for BrandeisU\n",
      "1279 . Importing tweets for Middlebury\n",
      "1280 . Imported tweets for WPIAdmissions\n",
      "1281 . Importing tweets for CNUcaptains\n",
      "1282 . Imported tweets for Concordia_MN\n",
      "1283 . Importing tweets for HWSColleges\n",
      "1284 . Imported tweets for gordoncollege\n",
      "1285 . Importing tweets for LawrenceUni\n",
      "1286 . Imported tweets for CentreC\n",
      "1287 . Importing tweets for SFCNY\n",
      "1288 . Imported tweets for AshlandInc\n",
      "1289 . Importing tweets for cardinalhealth\n",
      "1290 . Imported tweets for Chesapeake\n",
      "1291 . Importing tweets for DollarGeneral\n",
      "1292 . Imported tweets for FTI_US\n",
      "1293 . Importing tweets for TheHartford\n",
      "1294 . Imported tweets for IngramMicroInc\n",
      "1295 . Importing tweets for KellyServices\n",
      "1296 . Imported tweets for ManpowerGroup\n",
      "1297 . Importing tweets for hetmanrecovery\n",
      "1298 . Imported tweets for nwlrubbermaid\n",
      "1299 . Importing tweets for TerexCorp\n",
      "---- Persisting to CSV file\n",
      "1300 . Imported tweets for WasteManagement\n",
      "1301 . Importing tweets for pepsi\n",
      "1302 . Imported tweets for Pringles\n",
      "1303 . Importing tweets for BMWMotorsport\n",
      "1304 . Imported tweets for WaltDisneyWorld\n",
      "1305 . Importing tweets for dcshoes\n",
      "1306 . Imported tweets for Esp_Interativo\n",
      "1307 . Importing tweets for cnni\n",
      "1308 . Imported tweets for Wendys\n",
      "1309 . Importing tweets for astonmartin\n",
      "1310 . Imported tweets for htc\n",
      "1311 . Importing tweets for krispykreme\n",
      "1312 . Imported tweets for Toblerone\n",
      "1313 . Importing tweets for DICKS\n",
      "1314 . Imported tweets for MTVUK\n",
      "1315 . Importing tweets for Lowes\n",
      "1316 . Imported tweets for Cadillac\n",
      "1317 . Importing tweets for SamsClub\n",
      "1318 . Imported tweets for VanityFair\n",
      "1319 . Importing tweets for SingaporeAir\n",
      "1320 . Imported tweets for rolandgarros\n",
      "1321 . Importing tweets for SKECHERSUSA\n",
      "1322 . Imported tweets for GalatasaraySK\n",
      "1323 . Importing tweets for 49ers\n",
      "1324 . Imported tweets for BrooklynNets\n",
      "---- Persisting to CSV file\n",
      "1325 . Importing tweets for Lions\n",
      "1326 . Imported tweets for hornets\n",
      "1327 . Importing tweets for VanCanucks\n",
      "1328 . Imported tweets for Padres\n",
      "1329 . Importing tweets for DallasStars\n",
      "1330 . Imported tweets for impactmontreal\n",
      "1331 . Importing tweets for jackassworld\n",
      "1332 . Imported tweets for vdaymovie\n",
      "1333 . Importing tweets for secretariatfilm\n",
      "1334 . Imported tweets for FromPradaToNada\n",
      "1335 . Importing tweets for 127HoursMovie\n",
      "1336 . Imported tweets for TwoAndAHalfMen\n",
      "1337 . Importing tweets for BONESonFOX\n",
      "1338 . Imported tweets for UncleRuckusMovi\n",
      "1339 . Importing tweets for ABCFsecretlife\n",
      "1340 . Imported tweets for ScandalABC\n",
      "1341 . Importing tweets for Monk_USA\n",
      "1342 . Imported tweets for TheGoodWife_CBS\n",
      "1343 . Importing tweets for BlueMtnState\n",
      "1344 . Imported tweets for MajorCrimesTNT\n",
      "1345 . Importing tweets for wdytya\n",
      "1346 . Imported tweets for HotelHellonFOX\n",
      "1347 . Importing tweets for jeffprobstshow\n",
      "1348 . Imported tweets for MadeInJerseyCBS\n",
      "1349 . Importing tweets for BandHero\n",
      "---- Persisting to CSV file\n",
      "1350 . Imported tweets for YahooFinance\n",
      "1351 . Importing tweets for NowMag\n",
      "1352 . Imported tweets for TheBreastCancer\n",
      "1353 . Importing tweets for UofAlabama\n",
      "1354 . Imported tweets for Mizzou\n",
      "1355 . Importing tweets for LifeAtPurdue\n",
      "1356 . Imported tweets for USouthFlorida\n",
      "1357 . Importing tweets for ColoradoStateU\n",
      "1358 . Imported tweets for CarnegieMellon\n",
      "1359 . Importing tweets for ksunews\n",
      "1360 . Imported tweets for RITtigers\n",
      "1361 . Importing tweets for UNCClt_News\n",
      "1362 . Imported tweets for NDSUNews\n",
      "1363 . Importing tweets for waynestate\n",
      "1364 . Imported tweets for SantaClaraUniv\n",
      "1365 . Importing tweets for CityCollegeNY\n",
      "1366 . Imported tweets for MnSUMankatoAlum\n",
      "Unexpected error: <class 'urllib.error.URLError'>\n",
      "1368 . Imported tweets for Marist\n",
      "1369 . Importing tweets for oberlin_news\n",
      "1370 . Imported tweets for VisitSyracuse\n",
      "1371 . Importing tweets for cedarville\n",
      "1372 . Imported tweets for SusquehannaU\n",
      "1373 . Importing tweets for SMCAdmissions\n",
      "1374 . Imported tweets for NEWesleyan\n",
      "---- Persisting to CSV file\n",
      "1375 . Importing tweets for DruryUniversity\n",
      "1376 . Imported tweets for albioncollege\n",
      "1377 . Importing tweets for swarthmore\n",
      "1378 . Imported tweets for aspectinterior_\n",
      "1379 . Importing tweets for CarMax\n",
      "1380 . Imported tweets for CSX\n",
      "1381 . Importing tweets for DollarTree\n",
      "1382 . Imported tweets for SNLFinancial\n",
      "1383 . Importing tweets for KCCorp\n",
      "1384 . Imported tweets for MarathonOil\n",
      "1385 . Importing tweets for MicronTech\n",
      "1386 . Imported tweets for Newmont\n",
      "1387 . Importing tweets for Oracle\n",
      "1388 . Imported tweets for riteaid\n",
      "1389 . Importing tweets for TXInstruments\n",
      "1390 . Imported tweets for Walmart\n",
      "1391 . Importing tweets for Skittles\n",
      "1392 . Imported tweets for google\n",
      "1393 . Importing tweets for netflix\n",
      "1394 . Imported tweets for Barbie\n",
      "1395 . Importing tweets for Aeropostale\n",
      "1396 . Imported tweets for picnik\n",
      "1397 . Importing tweets for ChickfilA\n",
      "1398 . Imported tweets for GUESS\n",
      "1399 . Importing tweets for ATT\n",
      "---- Persisting to CSV file\n",
      "1400 . Imported tweets for DippinDots\n",
      "1401 . Importing tweets for WSJ\n",
      "1402 . Imported tweets for AmazonKindle\n",
      "1403 . Importing tweets for sonicdrivein\n",
      "1404 . Imported tweets for CBSNews\n",
      "1405 . Importing tweets for Playfish\n",
      "1406 . Imported tweets for LittleDebbie\n",
      "1407 . Importing tweets for CBSSports\n",
      "1408 . Imported tweets for WIRED\n",
      "1409 . Importing tweets for TGIFridays\n",
      "1410 . Imported tweets for MercedesAMGF1\n",
      "1411 . Importing tweets for cavs\n",
      "1412 . Imported tweets for Cubs\n",
      "1413 . Importing tweets for Redskins\n",
      "1414 . Imported tweets for ATLHawks\n",
      "1415 . Importing tweets for Mariners\n",
      "1416 . Imported tweets for RaysBaseball\n",
      "1417 . Importing tweets for AnaheimDucks\n",
      "1418 . Imported tweets for NERevolution\n",
      "1419 . Importing tweets for KarateKidMovie\n",
      "1420 . Imported tweets for SnowWhite\n",
      "1421 . Importing tweets for RepublicWadiya\n",
      "1422 . Imported tweets for gaslandmovie\n",
      "1423 . Importing tweets for CrimsonFilms\n",
      "1424 . Imported tweets for cartoonnetwork\n",
      "---- Persisting to CSV file\n",
      "1425 . Importing tweets for AmericanIdol\n",
      "1426 . Imported tweets for MythBusters\n",
      "1427 . Importing tweets for NCISLA_CBS\n",
      "1428 . Imported tweets for MASTERCHEFonFOX\n",
      "1429 . Importing tweets for cyborgturkey\n",
      "1430 . Imported tweets for _RevolutionTV\n",
      "1431 . Importing tweets for ABCFmelissajoey\n",
      "1432 . Imported tweets for HappyEndingsVH1\n",
      "1433 . Importing tweets for PerceptionTNT\n",
      "1434 . Imported tweets for OWN_Mystery\n",
      "1435 . Importing tweets for WholeTruthABC\n",
      "1436 . Imported tweets for UNITE\n",
      "1437 . Importing tweets for SuperMeatBoy\n",
      "1438 . Imported tweets for comingsoonnet\n",
      "1439 . Importing tweets for UGOdotcom\n",
      "1440 . Imported tweets for Harvard\n",
      "1441 . Importing tweets for universityofky\n",
      "1442 . Imported tweets for AF_Academy\n",
      "1443 . Importing tweets for USC\n",
      "1444 . Imported tweets for floridastate\n",
      "1445 . Importing tweets for VCUnews\n",
      "1446 . Imported tweets for myrwu\n",
      "1447 . Importing tweets for DrexelNews\n",
      "1448 . Imported tweets for OleMissRebels\n",
      "1449 . Importing tweets for utepnews\n",
      "---- Persisting to CSV file\n",
      "1450 . Imported tweets for UToledo\n",
      "1451 . Importing tweets for calpolypomona\n",
      "1452 . Imported tweets for bradleyu\n",
      "1453 . Importing tweets for SienaCollege\n",
      "1454 . Imported tweets for uidaho\n",
      "1455 . Importing tweets for UofStThomasMN\n",
      "1456 . Imported tweets for msudenver\n",
      "1457 . Importing tweets for FollowStevens\n",
      "1458 . Imported tweets for gettysburg\n",
      "1459 . Importing tweets for WilliamsCollege\n",
      "1460 . Imported tweets for BaldwinWallace\n",
      "1461 . Importing tweets for wittenberg\n",
      "1462 . Imported tweets for FrolicHawaii\n",
      "1463 . Importing tweets for NorthwesternMN\n",
      "1464 . Imported tweets for IL_Wesleyan\n",
      "1465 . Importing tweets for UofDallas\n",
      "1466 . Imported tweets for AmerenCorp\n",
      "1467 . Importing tweets for AssurantNews\n",
      "1468 . Imported tweets for BedBathBeyond\n",
      "1469 . Importing tweets for ChubbInsurance\n",
      "1470 . Imported tweets for Cummins\n",
      "1471 . Importing tweets for erie_insurance\n",
      "1472 . Imported tweets for FrontierCorp\n",
      "1473 . Importing tweets for HDSupplyFM\n",
      "1474 . Imported tweets for Microsoft\n",
      "---- Persisting to CSV file\n",
      "1475 . Importing tweets for newscorp\n",
      "1476 . Imported tweets for OshkoshDefense\n",
      "1477 . Importing tweets for PNCNews\n",
      "1478 . Imported tweets for RockTenn1\n",
      "1479 . Importing tweets for sprint\n",
      "1480 . Imported tweets for Textron\n",
      "1481 . Importing tweets for uhsinc_jobs\n",
      "1482 . Imported tweets for AppleMusic\n",
      "1483 . Importing tweets for Dove\n",
      "1484 . Imported tweets for googlechrome\n",
      "1485 . Importing tweets for Budweiser\n",
      "1486 . Imported tweets for dominos\n",
      "1487 . Importing tweets for MountainDew\n",
      "1488 . Imported tweets for armani\n",
      "1489 . Importing tweets for GameStop\n",
      "1490 . Imported tweets for AmericanExpress\n",
      "1491 . Importing tweets for Wikipedia\n",
      "1492 . Imported tweets for foxracing\n",
      "1493 . Importing tweets for NHL\n",
      "1494 . Imported tweets for lenovo\n",
      "1495 . Importing tweets for DrinkAriZona\n",
      "1496 . Imported tweets for marshalls\n",
      "1497 . Importing tweets for Toyota\n",
      "1498 . Imported tweets for Etsy\n",
      "1499 . Importing tweets for MazdaUSA\n",
      "---- Persisting to CSV file\n",
      "1500 . Imported tweets for GQMagazine\n",
      "1501 . Importing tweets for lanebryant\n",
      "1502 . Imported tweets for celtics\n",
      "1503 . Importing tweets for ChicagoBears\n",
      "1504 . Imported tweets for Ravens\n",
      "1505 . Importing tweets for nuggets\n",
      "1506 . Imported tweets for SacramentoKings\n",
      "1507 . Importing tweets for Orioles\n",
      "1508 . Imported tweets for NewYorkRedBulls\n",
      "1509 . Importing tweets for NHLFlames\n",
      "1510 . Imported tweets for WhitecapsFC\n",
      "1511 . Importing tweets for TheHungerGames\n",
      "1512 . Imported tweets for BattleshipMovie\n",
      "1513 . Importing tweets for SeeChernobyl\n",
      "1514 . Imported tweets for HowDoUKnowMovie\n",
      "1515 . Importing tweets for Brotherhoodfilm\n",
      "1516 . Imported tweets for iCarly\n",
      "1517 . Importing tweets for SportsCenter\n",
      "1518 . Imported tweets for Legendarios\n",
      "1519 . Importing tweets for Skins\n",
      "1520 . Imported tweets for HellsKitchenFOX\n",
      "1521 . Importing tweets for SHO_Homeland\n",
      "1522 . Imported tweets for CookingChannel\n",
      "1523 . Importing tweets for Bravotv\n",
      "1524 . Imported tweets for KNonFOX\n",
      "---- Persisting to CSV file\n",
      "1525 . Importing tweets for GCB\n",
      "1526 . Imported tweets for Detroit187ABC\n",
      "1527 . Importing tweets for supermodelme\n",
      "1528 . Imported tweets for CristinaCooks\n",
      "1529 . Importing tweets for Life_and_Style\n",
      "1530 . Imported tweets for POPSUGAR\n",
      "1531 . Importing tweets for NatEnquirer\n",
      "1532 . Imported tweets for RED\n",
      "1533 . Importing tweets for NASAJPL\n",
      "1534 . Imported tweets for Cornell\n",
      "1535 . Importing tweets for DiscoverAtlanta\n",
      "1536 . Imported tweets for ChooseChicago\n",
      "1537 . Importing tweets for csunorthridge\n",
      "1538 . Imported tweets for BYUIADMISSIONS\n",
      "1539 . Importing tweets for northerniowa\n",
      "1540 . Imported tweets for Fresno_State\n",
      "1541 . Importing tweets for ACUedu\n",
      "1542 . Imported tweets for TROYUnews\n",
      "1543 . Importing tweets for boisestatelive\n",
      "1544 . Imported tweets for UTSA\n",
      "1545 . Importing tweets for myUND\n",
      "1546 . Imported tweets for StJohnsU\n",
      "1547 . Importing tweets for Wellesley\n",
      "1548 . Imported tweets for StOlaf\n",
      "1549 . Importing tweets for mtholyoke\n",
      "---- Persisting to CSV file\n",
      "1550 . Imported tweets for StetsonU\n",
      "1551 . Importing tweets for saintjosephs\n",
      "1552 . Imported tweets for CarletonCollege\n",
      "1553 . Importing tweets for ManhattanEdu\n",
      "1554 . Imported tweets for IUPedu\n",
      "1555 . Importing tweets for alleghenycol\n",
      "1556 . Imported tweets for LVC\n",
      "1557 . Importing tweets for LinfieldCollege\n",
      "1558 . Imported tweets for AEPnews\n",
      "1559 . Importing tweets for Cigna\n",
      "1560 . Imported tweets for CVS_Extra\n",
      "1561 . Importing tweets for DowChemical\n",
      "1562 . Imported tweets for EsteeLauder\n",
      "1563 . Importing tweets for HealthMgmtAssoc\n",
      "1564 . Imported tweets for intel\n",
      "1565 . Importing tweets for kindredhealth\n",
      "1566 . Imported tweets for MarchbanksDDS\n",
      "1567 . Importing tweets for mjunction_india\n",
      "1568 . Imported tweets for PPGIndustries\n",
      "1569 . Importing tweets for ROKAutomation\n",
      "1570 . Imported tweets for StJudeMedicalIR\n",
      "1571 . Importing tweets for unumnews\n",
      "1572 . Imported tweets for WellsFargo\n",
      "1573 . Importing tweets for Skype\n",
      "1574 . Imported tweets for MonsterEnergy\n",
      "---- Persisting to CSV file\n",
      "1575 . Importing tweets for LouisVuitton_US\n",
      "1576 . Imported tweets for DisneyPixar\n",
      "1577 . Importing tweets for Yahoo\n",
      "1578 . Imported tweets for Olympics\n",
      "1579 . Importing tweets for Dell\n",
      "1580 . Imported tweets for HUGOBOSS\n",
      "1581 . Importing tweets for LancomeUSA\n",
      "1582 . Imported tweets for havaianas\n",
      "1583 . Importing tweets for TeenVogue\n",
      "1584 . Imported tweets for InStyle\n",
      "1585 . Importing tweets for bing\n",
      "1586 . Imported tweets for YahooMovies\n",
      "1587 . Importing tweets for Hanes\n",
      "1588 . Imported tweets for StarCraft\n",
      "1589 . Importing tweets for buildabear\n",
      "1590 . Imported tweets for jetairways\n",
      "1591 . Importing tweets for AmericanAir\n",
      "1592 . Imported tweets for hkogame\n",
      "1593 . Importing tweets for TheEllenShow\n",
      "1594 . Imported tweets for Yankees\n",
      "1595 . Importing tweets for Saints\n",
      "1596 . Imported tweets for tigers\n",
      "1597 . Importing tweets for AtlantaFalcons\n",
      "1598 . Imported tweets for MapleLeafs\n",
      "1599 . Importing tweets for Bucks\n",
      "---- Persisting to CSV file\n",
      "1600 . Imported tweets for washcaps\n",
      "1601 . Importing tweets for CaterhamF1\n",
      "1602 . Imported tweets for dcunited\n",
      "1603 . Importing tweets for StepUpMovie\n",
      "1604 . Imported tweets for PredatorsMovie\n",
      "1605 . Importing tweets for LoveOtherDrugs\n",
      "1606 . Imported tweets for PrometheusFilms\n",
      "1607 . Importing tweets for dofafilm\n",
      "1608 . Imported tweets for OfficialHIMYM\n",
      "1609 . Importing tweets for BreakingBad_AMC\n",
      "1610 . Imported tweets for LastFallMovie\n",
      "1611 . Importing tweets for lhhhollywood\n",
      "1612 . Imported tweets for BurnNotice_USA\n",
      "1613 . Importing tweets for ABCFmiobi\n",
      "1614 . Imported tweets for nbcparenthood\n",
      "1615 . Importing tweets for Elementary_CBS\n",
      "1616 . Imported tweets for SHO_realLword\n",
      "1617 . Importing tweets for HumanTargetFOX\n",
      "1618 . Imported tweets for NBCGuyswithKids\n",
      "1619 . Importing tweets for OWN_MasterClass\n",
      "1620 . Imported tweets for LastResort_ABC\n",
      "1621 . Importing tweets for usweekly\n",
      "1622 . Imported tweets for POPSUGAREnt\n",
      "1623 . Importing tweets for star_magazine\n",
      "1624 . Imported tweets for wikileaks\n",
      "---- Persisting to CSV file\n",
      "1625 . Importing tweets for UCLA\n",
      "1626 . Imported tweets for UWMadison\n",
      "1627 . Importing tweets for BU_Tweets\n",
      "1628 . Imported tweets for NotreDame\n",
      "1629 . Importing tweets for MasonSpirit\n",
      "1630 . Imported tweets for biolau\n",
      "1631 . Importing tweets for ohiou\n",
      "1632 . Imported tweets for csuf\n",
      "1633 . Importing tweets for UNM\n",
      "1634 . Imported tweets for EKUStories\n",
      "1635 . Importing tweets for UNCWilmington\n",
      "1636 . Imported tweets for LoyolaChicago\n",
      "1637 . Importing tweets for CUDenver\n",
      "1638 . Imported tweets for RowanUniversity\n",
      "1639 . Importing tweets for UMassLowell\n",
      "1640 . Imported tweets for UPortland\n",
      "1641 . Importing tweets for rollinscollege\n",
      "1642 . Imported tweets for CatholicUniv\n",
      "1643 . Importing tweets for ColoradoCollege\n",
      "1644 . Imported tweets for HoughtonCollege\n",
      "1645 . Importing tweets for WheatonCollege\n",
      "1646 . Imported tweets for univpugetsound\n",
      "1647 . Importing tweets for HanoverCollege\n",
      "1648 . Imported tweets for UMMorris\n",
      "Unexpected error: <class 'IndexError'>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "complete_profiles = import_dict_from_csv('data/csv/extracted_complete_twitter_actor_info.csv')\n",
    "if len(complete_profiles) > 0:\n",
    "    print('Loading complete profiles from csv file...')\n",
    "    brand_profiles = complete_profiles\n",
    "\n",
    "def persistToFile(count, data):\n",
    "    if count % 25 == 0:\n",
    "            export_extracted_data_to_csv(data, 'data/csv/extracted_complete_twitter_actor_info.csv')\n",
    "            print('---- Persisting to CSV file')\n",
    "    \n",
    "i = 0\n",
    "for actor in brand_profiles:\n",
    "    try:\n",
    "        i += 1\n",
    "        if (not 'tweet' in actor) or (not actor['tweet']):\n",
    "            # Retrieve their last 'posted' tweet to use in the feature selection\n",
    "            actor_tweets = t.statuses.user_timeline(screen_name=actor['screen_name'])\n",
    "            brand_profiles[i]['tweets'] = actor_tweets\n",
    "            actor_tweet = actor_tweets[0] if actor_tweets else None\n",
    "            brand_profiles[i]['tweet'] = actor_tweet\n",
    "            persistToFile(i, brand_profiles)\n",
    "            print(i, '. Importing tweets for', actor['screen_name'])\n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            persistToFile(i, brand_profiles)\n",
    "            print(i, '. Imported tweets for', actor['screen_name'])\n",
    "    except:\n",
    "         print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "\n",
    "export_extracted_data_to_csv(brand_profiles, 'data/csv/extracted_complete_twitter_actor_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in a csv file with default format for trainning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already writed 100 tweets to CSV\n",
      "Already writed 200 tweets to CSV\n",
      "Already writed 300 tweets to CSV\n",
      "Already writed 400 tweets to CSV\n",
      "Already writed 500 tweets to CSV\n",
      "Already writed 600 tweets to CSV\n",
      "Already writed 700 tweets to CSV\n",
      "Already writed 800 tweets to CSV\n",
      "Already writed 900 tweets to CSV\n",
      "Already writed 1000 tweets to CSV\n",
      "100 errors saving the tweets to CSV\n",
      "Already writed 1100 tweets to CSV\n",
      "200 errors saving the tweets to CSV\n",
      "Already writed 1200 tweets to CSV\n",
      "300 errors saving the tweets to CSV\n",
      "Already writed 1300 tweets to CSV\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "from html import escape\n",
    "\n",
    "s = 0\n",
    "e = 0\n",
    "brand_profiles = import_dict_from_csv('data/csv/extracted_complete_twitter_actor_info.csv')\n",
    "with open('data/csv/brand_trainned.csv', 'w') as csv_file:\n",
    "    tweets_writer = csv.writer(csv_file)\n",
    "    tweets_writer.writerow([\n",
    "        'actor_id',\n",
    "        'actor_screen_name',\n",
    "        'actor_name',\n",
    "        'actor_verified',\n",
    "        'actor_friends_count',\n",
    "        'actor_followers_count',\n",
    "        'actor_listed_count',\n",
    "        'actor_statuses_count',\n",
    "        'actor_favorites_count',\n",
    "        'actor_summary',\n",
    "        'actor_created_at',\n",
    "        'actor_location',\n",
    "        \n",
    "        'tweet_id',\n",
    "        'tweet_created_at',\n",
    "        'tweet_generator',\n",
    "        'tweet_body',\n",
    "        'tweet_verb',\n",
    "            \n",
    "        'tweet_urls_count',\n",
    "        'tweet_mentions_count',\n",
    "        'tweet_hashtags_count',\n",
    "        'tweet_trends_count',\n",
    "        'tweet_symbols_count'])\n",
    "    for profile in brand_profiles:\n",
    "        try:\n",
    "            tweet = ast.literal_eval((str(profile['tweet'])))\n",
    "            tweets_writer.writerow([\n",
    "                    tweet['user']['id'],\n",
    "                    tweet['user']['screen_name'],\n",
    "                    tweet['user']['name'],\n",
    "                    tweet['user']['verified'],\n",
    "                    tweet['user']['friends_count'],\n",
    "                    tweet['user']['followers_count'],\n",
    "                    tweet['user']['listed_count'],\n",
    "                    tweet['user']['statuses_count'],\n",
    "                    tweet['user']['favourites_count'],\n",
    "                    tweet['user']['description'],\n",
    "                    tweet['user']['created_at'],\n",
    "                    tweet['user']['location'] if tweet['user'].get('location') else 'null',\n",
    "\n",
    "                    tweet['id'],\n",
    "                    tweet['created_at'],\n",
    "                    re.findall('>(.*)<', tweet['source'])[0],\n",
    "                    tweet['text'],\n",
    "                    not tweet['retweeted'],\n",
    "                    len(tweet['entities']['urls']),\n",
    "                    len(tweet['entities']['user_mentions']),\n",
    "                    len(tweet['entities']['hashtags']),\n",
    "                    \"\",\n",
    "                    len(tweet['entities']['symbols'])\n",
    "                ])\n",
    "            s += 1\n",
    "            if s % 100 == 0:\n",
    "                print('Already writed', s, 'tweets to CSV')\n",
    "            \n",
    "        except:\n",
    "            e += 1\n",
    "            if e % 100 == 0:\n",
    "                print(e, 'errors saving the tweets to CSV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sensing the business trainned data\n",
    "It is important to take a look at some characterístics of this data, since we will build the trainner on top of it, plus the researches already analysed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_screen_name</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>actor_verified</th>\n",
       "      <th>actor_friends_count</th>\n",
       "      <th>actor_followers_count</th>\n",
       "      <th>actor_listed_count</th>\n",
       "      <th>actor_statuses_count</th>\n",
       "      <th>actor_favorites_count</th>\n",
       "      <th>actor_summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>tweet_generator</th>\n",
       "      <th>tweet_body</th>\n",
       "      <th>tweet_verb</th>\n",
       "      <th>tweet_urls_count</th>\n",
       "      <th>tweet_mentions_count</th>\n",
       "      <th>tweet_hashtags_count</th>\n",
       "      <th>tweet_trends_count</th>\n",
       "      <th>tweet_symbols_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 22919665</td>\n",
       "      <td>     AirAsia</td>\n",
       "      <td>              AirAsia</td>\n",
       "      <td> True</td>\n",
       "      <td>   639</td>\n",
       "      <td> 1537751</td>\n",
       "      <td> 4916</td>\n",
       "      <td> 35601</td>\n",
       "      <td>    37</td>\n",
       "      <td> Welcome to our official Twitter account where ...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614395234659635200</td>\n",
       "      <td> Fri Jun 26 11:30:13 +0000 2015</td>\n",
       "      <td>        Twitter Ads</td>\n",
       "      <td> 为明年计划个周末出游吧！吉隆坡 - 新加坡，RM37起！快来抢购哦！http://t.co/...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 85399171</td>\n",
       "      <td>      tvland</td>\n",
       "      <td>              TV Land</td>\n",
       "      <td> True</td>\n",
       "      <td> 18100</td>\n",
       "      <td>   48050</td>\n",
       "      <td>  795</td>\n",
       "      <td> 29934</td>\n",
       "      <td> 11320</td>\n",
       "      <td> @GaffiganShow, @ImpastorTV &amp; @The_Exes start W...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614312317313019904</td>\n",
       "      <td> Fri Jun 26 06:00:44 +0000 2015</td>\n",
       "      <td>          Hootsuite</td>\n",
       "      <td> Nancy comes out on this episode of #Roseanne #...</td>\n",
       "      <td> True</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 18360370</td>\n",
       "      <td>    utahjazz</td>\n",
       "      <td>            Utah Jazz</td>\n",
       "      <td> True</td>\n",
       "      <td>   331</td>\n",
       "      <td>  365180</td>\n",
       "      <td> 3838</td>\n",
       "      <td> 28392</td>\n",
       "      <td>  3911</td>\n",
       "      <td> Official Twitter account of the Utah Jazz. Get...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614309402397573120</td>\n",
       "      <td> Fri Jun 26 05:49:09 +0000 2015</td>\n",
       "      <td>   Tweetbot for iΟS</td>\n",
       "      <td> RT @TreyMambaLyles: Dreams do come true!!! Tha...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 52803520</td>\n",
       "      <td>      astros</td>\n",
       "      <td>          #VoteAltuve</td>\n",
       "      <td> True</td>\n",
       "      <td>   423</td>\n",
       "      <td>  228291</td>\n",
       "      <td> 2799</td>\n",
       "      <td> 32980</td>\n",
       "      <td>   308</td>\n",
       "      <td> The Official Twitter of the Houston Astros. Ru...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614293312820740096</td>\n",
       "      <td> Fri Jun 26 04:45:13 +0000 2015</td>\n",
       "      <td>      Adobe® Social</td>\n",
       "      <td> Career high in #whiff(s) for @kidkeuchy … and ...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 28173550</td>\n",
       "      <td> TBLightning</td>\n",
       "      <td> Tampa Bay Lightning </td>\n",
       "      <td> True</td>\n",
       "      <td>  6976</td>\n",
       "      <td>  275362</td>\n",
       "      <td> 3590</td>\n",
       "      <td> 52064</td>\n",
       "      <td>  2215</td>\n",
       "      <td> Official Twitter of the 2015 Eastern Conferenc...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614234380534464512</td>\n",
       "      <td> Fri Jun 26 00:51:02 +0000 2015</td>\n",
       "      <td> Twitter for iPhone</td>\n",
       "      <td> RT @RHiggins_TBSC: THANK YOU to all who made t...</td>\n",
       "      <td> True</td>\n",
       "      <td> 0</td>\n",
       "      <td> 2</td>\n",
       "      <td> 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   actor_id actor_screen_name            actor_name actor_verified  \\\n",
       "0  22919665           AirAsia               AirAsia           True   \n",
       "1  85399171            tvland               TV Land           True   \n",
       "2  18360370          utahjazz             Utah Jazz           True   \n",
       "3  52803520            astros           #VoteAltuve           True   \n",
       "4  28173550       TBLightning  Tampa Bay Lightning            True   \n",
       "\n",
       "   actor_friends_count  actor_followers_count  actor_listed_count  \\\n",
       "0                  639                1537751                4916   \n",
       "1                18100                  48050                 795   \n",
       "2                  331                 365180                3838   \n",
       "3                  423                 228291                2799   \n",
       "4                 6976                 275362                3590   \n",
       "\n",
       "   actor_statuses_count  actor_favorites_count  \\\n",
       "0                 35601                     37   \n",
       "1                 29934                  11320   \n",
       "2                 28392                   3911   \n",
       "3                 32980                    308   \n",
       "4                 52064                   2215   \n",
       "\n",
       "                                       actor_summary         ...           \\\n",
       "0  Welcome to our official Twitter account where ...         ...            \n",
       "1  @GaffiganShow, @ImpastorTV & @The_Exes start W...         ...            \n",
       "2  Official Twitter account of the Utah Jazz. Get...         ...            \n",
       "3  The Official Twitter of the Houston Astros. Ru...         ...            \n",
       "4  Official Twitter of the 2015 Eastern Conferenc...         ...            \n",
       "\n",
       "             tweet_id                tweet_created_at     tweet_generator  \\\n",
       "0  614395234659635200  Fri Jun 26 11:30:13 +0000 2015         Twitter Ads   \n",
       "1  614312317313019904  Fri Jun 26 06:00:44 +0000 2015           Hootsuite   \n",
       "2  614309402397573120  Fri Jun 26 05:49:09 +0000 2015    Tweetbot for iΟS   \n",
       "3  614293312820740096  Fri Jun 26 04:45:13 +0000 2015       Adobe® Social   \n",
       "4  614234380534464512  Fri Jun 26 00:51:02 +0000 2015  Twitter for iPhone   \n",
       "\n",
       "                                          tweet_body tweet_verb  \\\n",
       "0  为明年计划个周末出游吧！吉隆坡 - 新加坡，RM37起！快来抢购哦！http://t.co/...       True   \n",
       "1  Nancy comes out on this episode of #Roseanne #...       True   \n",
       "2  RT @TreyMambaLyles: Dreams do come true!!! Tha...       True   \n",
       "3  Career high in #whiff(s) for @kidkeuchy … and ...       True   \n",
       "4  RT @RHiggins_TBSC: THANK YOU to all who made t...       True   \n",
       "\n",
       "  tweet_urls_count tweet_mentions_count  tweet_hashtags_count  \\\n",
       "0                1                    0                     0   \n",
       "1                0                    0                     2   \n",
       "2                1                    1                     1   \n",
       "3                1                    2                     1   \n",
       "4                0                    2                     2   \n",
       "\n",
       "   tweet_trends_count  tweet_symbols_count  \n",
       "0                 NaN                    0  \n",
       "1                 NaN                    0  \n",
       "2                 NaN                    0  \n",
       "3                 NaN                    0  \n",
       "4                 NaN                    0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tweets = pd.read_csv('data/csv/brand_trainned.csv')\n",
    "df_tweets = df_tweets.dropna(subset=['actor_summary', 'tweet_generator'])\n",
    "print(len(df_tweets))\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     percentage\n",
      "tweet_generator                \n",
      "Twitter Web Client    29.960784\n",
      "Hootsuite             16.627451\n",
      "TweetDeck             13.568627\n",
      "Twitter for iPhone    11.215686\n",
      "Sprinklr               4.862745\n",
      "Adobe® Social          2.431373\n",
      "Sprout Social          2.274510\n",
      "SocialFlow             2.039216\n",
      "Spredfast app          1.568627\n",
      "Twitter for Android    1.019608\n"
     ]
    }
   ],
   "source": [
    "device = df_tweets[['tweet_generator', 'tweet_id']]\n",
    "posts_by_device = device.groupby('tweet_generator').count()\n",
    "posts_by_device['percentage'] = (posts_by_device.tweet_id / posts_by_device.tweet_id.sum()) * 100\n",
    "posts_by_device = posts_by_device[['percentage']].sort('percentage', ascending=False)\n",
    "\n",
    "print(posts_by_device.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actor_id</th>\n",
       "      <th>actor_screen_name</th>\n",
       "      <th>actor_name</th>\n",
       "      <th>actor_verified</th>\n",
       "      <th>actor_friends_count</th>\n",
       "      <th>actor_followers_count</th>\n",
       "      <th>actor_listed_count</th>\n",
       "      <th>actor_statuses_count</th>\n",
       "      <th>actor_favorites_count</th>\n",
       "      <th>actor_summary</th>\n",
       "      <th>...</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>tweet_generator</th>\n",
       "      <th>tweet_body</th>\n",
       "      <th>tweet_verb</th>\n",
       "      <th>tweet_urls_count</th>\n",
       "      <th>tweet_mentions_count</th>\n",
       "      <th>tweet_hashtags_count</th>\n",
       "      <th>tweet_trends_count</th>\n",
       "      <th>tweet_symbols_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8 </th>\n",
       "      <td> 343454456</td>\n",
       "      <td>    TotalRecall</td>\n",
       "      <td>   Total Recall</td>\n",
       "      <td>  True</td>\n",
       "      <td>  30</td>\n",
       "      <td>    3651</td>\n",
       "      <td>   50</td>\n",
       "      <td>  310</td>\n",
       "      <td>    0</td>\n",
       "      <td> Total Recall is out on DVD and Director’s Cut ...</td>\n",
       "      <td>...</td>\n",
       "      <td> 306856395926544385</td>\n",
       "      <td> Wed Feb 27 20:00:40 +0000 2013</td>\n",
       "      <td> Twitter Web Client</td>\n",
       "      <td> Have u played the single player demo of God of...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 </th>\n",
       "      <td> 545373621</td>\n",
       "      <td>    hopesprings</td>\n",
       "      <td>   Hope Springs</td>\n",
       "      <td> False</td>\n",
       "      <td> 187</td>\n",
       "      <td>     902</td>\n",
       "      <td>   30</td>\n",
       "      <td>  284</td>\n",
       "      <td>   30</td>\n",
       "      <td> Follow the official HOPE SPRINGS Twitter page ...</td>\n",
       "      <td>...</td>\n",
       "      <td> 289873538586927104</td>\n",
       "      <td> Fri Jan 11 23:16:52 +0000 2013</td>\n",
       "      <td> Twitter Web Client</td>\n",
       "      <td> #HopeSprings' Meryl Streep won the @PeoplesCho...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td> 113707000</td>\n",
       "      <td> BudClaymanOC87</td>\n",
       "      <td>    Bud Clayman</td>\n",
       "      <td> False</td>\n",
       "      <td> 228</td>\n",
       "      <td>     173</td>\n",
       "      <td>    4</td>\n",
       "      <td>   78</td>\n",
       "      <td>    0</td>\n",
       "      <td> Can you make a movie while having mental illne...</td>\n",
       "      <td>...</td>\n",
       "      <td>        24953461731</td>\n",
       "      <td> Sun Sep 19 17:40:59 +0000 2010</td>\n",
       "      <td> Twitter Web Client</td>\n",
       "      <td> Just posted on F.B. that we had a great time a...</td>\n",
       "      <td> True</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>  23504870</td>\n",
       "      <td>      SouthPark</td>\n",
       "      <td>    South Park </td>\n",
       "      <td>  True</td>\n",
       "      <td> 260</td>\n",
       "      <td> 1710640</td>\n",
       "      <td> 5394</td>\n",
       "      <td> 9864</td>\n",
       "      <td> 3408</td>\n",
       "      <td> The official South Park twitter.  Watch full e...</td>\n",
       "      <td>...</td>\n",
       "      <td> 614175793514549248</td>\n",
       "      <td> Thu Jun 25 20:58:14 +0000 2015</td>\n",
       "      <td> Twitter Web Client</td>\n",
       "      <td> No it wasn't me.  IT WAS THE SPOOKY GHOST!!! #...</td>\n",
       "      <td> True</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>   9786732</td>\n",
       "      <td> ghostwhisperer</td>\n",
       "      <td> ghostwhisperer</td>\n",
       "      <td> False</td>\n",
       "      <td> 102</td>\n",
       "      <td>   15413</td>\n",
       "      <td>  279</td>\n",
       "      <td> 1467</td>\n",
       "      <td>   11</td>\n",
       "      <td> The dead are still talking...  and she's still...</td>\n",
       "      <td>...</td>\n",
       "      <td> 603195879902867456</td>\n",
       "      <td> Tue May 26 13:47:59 +0000 2015</td>\n",
       "      <td> Twitter Web Client</td>\n",
       "      <td> #GhostWhisperer producers &amp;amp; creator @JThom...</td>\n",
       "      <td> True</td>\n",
       "      <td> 1</td>\n",
       "      <td> 2</td>\n",
       "      <td> 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actor_id actor_screen_name      actor_name actor_verified  \\\n",
       "8   343454456       TotalRecall    Total Recall           True   \n",
       "9   545373621       hopesprings    Hope Springs          False   \n",
       "10  113707000    BudClaymanOC87     Bud Clayman          False   \n",
       "11   23504870         SouthPark     South Park            True   \n",
       "14    9786732    ghostwhisperer  ghostwhisperer          False   \n",
       "\n",
       "    actor_friends_count  actor_followers_count  actor_listed_count  \\\n",
       "8                    30                   3651                  50   \n",
       "9                   187                    902                  30   \n",
       "10                  228                    173                   4   \n",
       "11                  260                1710640                5394   \n",
       "14                  102                  15413                 279   \n",
       "\n",
       "    actor_statuses_count  actor_favorites_count  \\\n",
       "8                    310                      0   \n",
       "9                    284                     30   \n",
       "10                    78                      0   \n",
       "11                  9864                   3408   \n",
       "14                  1467                     11   \n",
       "\n",
       "                                        actor_summary         ...           \\\n",
       "8   Total Recall is out on DVD and Director’s Cut ...         ...            \n",
       "9   Follow the official HOPE SPRINGS Twitter page ...         ...            \n",
       "10  Can you make a movie while having mental illne...         ...            \n",
       "11  The official South Park twitter.  Watch full e...         ...            \n",
       "14  The dead are still talking...  and she's still...         ...            \n",
       "\n",
       "              tweet_id                tweet_created_at     tweet_generator  \\\n",
       "8   306856395926544385  Wed Feb 27 20:00:40 +0000 2013  Twitter Web Client   \n",
       "9   289873538586927104  Fri Jan 11 23:16:52 +0000 2013  Twitter Web Client   \n",
       "10         24953461731  Sun Sep 19 17:40:59 +0000 2010  Twitter Web Client   \n",
       "11  614175793514549248  Thu Jun 25 20:58:14 +0000 2015  Twitter Web Client   \n",
       "14  603195879902867456  Tue May 26 13:47:59 +0000 2015  Twitter Web Client   \n",
       "\n",
       "                                           tweet_body tweet_verb  \\\n",
       "8   Have u played the single player demo of God of...       True   \n",
       "9   #HopeSprings' Meryl Streep won the @PeoplesCho...       True   \n",
       "10  Just posted on F.B. that we had a great time a...       True   \n",
       "11  No it wasn't me.  IT WAS THE SPOOKY GHOST!!! #...       True   \n",
       "14  #GhostWhisperer producers &amp; creator @JThom...       True   \n",
       "\n",
       "   tweet_urls_count tweet_mentions_count  tweet_hashtags_count  \\\n",
       "8                 1                    0                     1   \n",
       "9                 1                    1                     1   \n",
       "10                0                    0                     0   \n",
       "11                0                    0                     1   \n",
       "14                1                    2                     3   \n",
       "\n",
       "    tweet_trends_count  tweet_symbols_count  \n",
       "8                  NaN                    0  \n",
       "9                  NaN                    0  \n",
       "10                 NaN                    0  \n",
       "11                 NaN                    0  \n",
       "14                 NaN                    0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[df_tweets['tweet_generator'] == 'Twitter Web Client'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
