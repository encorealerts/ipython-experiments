{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Initializing input dataframe: \n",
      "(20245, 13)\n",
      "==> Concatenating dataframe from actor_classification_train_copy.csv: \n",
      "(20245, 13)\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'data/manually_categorized/'\n",
    "input_prefix = 'actor_classification_train'\n",
    "train = None\n",
    "for file in os.listdir(input_dir):\n",
    "  if fnmatch.fnmatch(file, input_prefix+'*.csv'):\n",
    "    if train is None:\n",
    "      print \"==> Initializing input dataframe: \"\n",
    "      train = pd.read_csv(open(input_dir+file,'rU'),\n",
    "                          engine='python', sep=\",\", quoting=1)\n",
    "    else:\n",
    "      print \"==> Concatenating dataframe from \" + file + \": \"\n",
    "      train = pd.concat([train, pd.read_csv(open(input_dir+file,'rU'),\n",
    "                          engine='python', sep=\",\", quoting=1)])\n",
    "    train.drop_duplicates(inplace=True)\n",
    "    print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>summary</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>link</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>segment</th>\n",
       "      <th>manual_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Guy</td>\n",
       "      <td>ZZ0</td>\n",
       "      <td>en</td>\n",
       "      <td>394</td>\n",
       "      <td>14626</td>\n",
       "      <td>122072</td>\n",
       "      <td>Martial arts, contortion, 7-string elec violin...</td>\n",
       "      <td>122030</td>\n",
       "      <td>http://www.twitter.com/ZZ0</td>\n",
       "      <td>745</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>party here</td>\n",
       "      <td>zxynisgod</td>\n",
       "      <td>es</td>\n",
       "      <td>75357</td>\n",
       "      <td>169818</td>\n",
       "      <td>44087</td>\n",
       "      <td>���I hate One Direction.�� -people who have lo...</td>\n",
       "      <td>72756</td>\n",
       "      <td>http://www.twitter.com/zxynisgod</td>\n",
       "      <td>327</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?��</td>\n",
       "      <td>Zxntio</td>\n",
       "      <td>en</td>\n",
       "      <td>24372</td>\n",
       "      <td>38662</td>\n",
       "      <td>118</td>\n",
       "      <td>@rantzantio</td>\n",
       "      <td>119602</td>\n",
       "      <td>http://www.twitter.com/Zxntio</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>business</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>�_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_</td>\n",
       "      <td>zxkia</td>\n",
       "      <td>en-gb</td>\n",
       "      <td>9874</td>\n",
       "      <td>119158</td>\n",
       "      <td>127928</td>\n",
       "      <td>Don't take me seriously. || Turn off rts &amp; tur...</td>\n",
       "      <td>197890</td>\n",
       "      <td>http://www.twitter.com/zxkia</td>\n",
       "      <td>170</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Zxkia</td>\n",
       "      <td>en</td>\n",
       "      <td>94</td>\n",
       "      <td>5514</td>\n",
       "      <td>12563</td>\n",
       "      <td>Somewhere between I want it and I got it. ~ Pr...</td>\n",
       "      <td>24316</td>\n",
       "      <td>http://twitter.com/Zxkia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       name screen_name   lang  \\\n",
       "0                                       Guy         ZZ0     en   \n",
       "1                               party here    zxynisgod     es   \n",
       "2                                       ?��      Zxntio     en   \n",
       "3  �_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_܃_       zxkia  en-gb   \n",
       "4                                       NaN       Zxkia     en   \n",
       "\n",
       "   favourites_count  statuses_count  friends_count  \\\n",
       "0               394           14626         122072   \n",
       "1             75357          169818          44087   \n",
       "2             24372           38662            118   \n",
       "3              9874          119158         127928   \n",
       "4                94            5514          12563   \n",
       "\n",
       "                                             summary  followers_count  \\\n",
       "0  Martial arts, contortion, 7-string elec violin...           122030   \n",
       "1  ���I hate One Direction.�� -people who have lo...            72756   \n",
       "2                                        @rantzantio           119602   \n",
       "3  Don't take me seriously. || Turn off rts & tur...           197890   \n",
       "4  Somewhere between I want it and I got it. ~ Pr...            24316   \n",
       "\n",
       "                               link  listed_count verified   segment  \\\n",
       "0        http://www.twitter.com/ZZ0           745    False    person   \n",
       "1  http://www.twitter.com/zxynisgod           327    False    person   \n",
       "2     http://www.twitter.com/Zxntio             5    False  business   \n",
       "3      http://www.twitter.com/zxkia           170    False    person   \n",
       "4          http://twitter.com/Zxkia           NaN      NaN       NaN   \n",
       "\n",
       "   manual_segment  \n",
       "0               0  \n",
       "1               1  \n",
       "2               1  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>summary</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>link</th>\n",
       "      <th>listed_count</th>\n",
       "      <th>verified</th>\n",
       "      <th>segment</th>\n",
       "      <th>manual_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20246</th>\n",
       "      <td>DOSE</td>\n",
       "      <td>___Dose___</td>\n",
       "      <td>en</td>\n",
       "      <td>8</td>\n",
       "      <td>20194</td>\n",
       "      <td>12</td>\n",
       "      <td>A Strong Dose of Amazing People, Places, and T...</td>\n",
       "      <td>421003</td>\n",
       "      <td>http://www.twitter.com/___Dose___</td>\n",
       "      <td>2949</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20247</th>\n",
       "      <td>One Direction News</td>\n",
       "      <td>_______1d_4ever</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>18578</td>\n",
       "      <td>37552</td>\n",
       "      <td>All the latest One Direction news from around ...</td>\n",
       "      <td>46205</td>\n",
       "      <td>http://www.twitter.com/_______1d_4ever</td>\n",
       "      <td>46</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20248</th>\n",
       "      <td>Tyrne Clark</td>\n",
       "      <td>10223335</td>\n",
       "      <td>en</td>\n",
       "      <td>467</td>\n",
       "      <td>13481</td>\n",
       "      <td>60692</td>\n",
       "      <td>Shouldn't a strange and wonderful world be ful...</td>\n",
       "      <td>58620</td>\n",
       "      <td>http://www.twitter.com/10223335</td>\n",
       "      <td>54</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20249</th>\n",
       "      <td>1776</td>\n",
       "      <td>1776</td>\n",
       "      <td>en</td>\n",
       "      <td>6586</td>\n",
       "      <td>9746</td>\n",
       "      <td>1293</td>\n",
       "      <td>Global incubator &amp; seed fund helping startups ...</td>\n",
       "      <td>87459</td>\n",
       "      <td>http://www.twitter.com/1776</td>\n",
       "      <td>1125</td>\n",
       "      <td>False</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20250</th>\n",
       "      <td>350 dot org</td>\n",
       "      <td>350</td>\n",
       "      <td>en</td>\n",
       "      <td>1153</td>\n",
       "      <td>25876</td>\n",
       "      <td>19502</td>\n",
       "      <td>Join a global movement that's inspiring the wo...</td>\n",
       "      <td>266424</td>\n",
       "      <td>http://www.twitter.com/350</td>\n",
       "      <td>5870</td>\n",
       "      <td>True</td>\n",
       "      <td>person</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name      screen_name lang  favourites_count  \\\n",
       "20246                DOSE       ___Dose___   en                 8   \n",
       "20247  One Direction News  _______1d_4ever   en                 0   \n",
       "20248         Tyrne Clark         10223335   en               467   \n",
       "20249                1776             1776   en              6586   \n",
       "20250         350 dot org              350   en              1153   \n",
       "\n",
       "       statuses_count  friends_count  \\\n",
       "20246           20194             12   \n",
       "20247           18578          37552   \n",
       "20248           13481          60692   \n",
       "20249            9746           1293   \n",
       "20250           25876          19502   \n",
       "\n",
       "                                                 summary  followers_count  \\\n",
       "20246  A Strong Dose of Amazing People, Places, and T...           421003   \n",
       "20247  All the latest One Direction news from around ...            46205   \n",
       "20248  Shouldn't a strange and wonderful world be ful...            58620   \n",
       "20249  Global incubator & seed fund helping startups ...            87459   \n",
       "20250  Join a global movement that's inspiring the wo...           266424   \n",
       "\n",
       "                                         link  listed_count verified segment  \\\n",
       "20246       http://www.twitter.com/___Dose___          2949    False  person   \n",
       "20247  http://www.twitter.com/_______1d_4ever            46    False  person   \n",
       "20248         http://www.twitter.com/10223335            54    False  person   \n",
       "20249             http://www.twitter.com/1776          1125    False  person   \n",
       "20250              http://www.twitter.com/350          5870     True  person   \n",
       "\n",
       "       manual_segment  \n",
       "20246               0  \n",
       "20247               0  \n",
       "20248               1  \n",
       "20249               0  \n",
       "20250               0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer with LabelEncoder for 'verified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VerifiedTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X.verified.fillna(False, inplace=True)\n",
    "        X.verified = LabelEncoder().fit_transform(X.verified)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    17337\n",
      "1     2908\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "verified_transformer = VerifiedTransformer()\n",
    "verified_transformer.transform(train)\n",
    "\n",
    "print train.verified.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom OneHotEncoding for lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LangOneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        valid_langs = list(set(X.lang) - set([None, np.nan, 'Select Language...']))\n",
    "        self.feature_names_ = [\"lang_\"+l for l in valid_langs]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self, 'feature_names_')\n",
    "        \n",
    "        X[\"lang\"].fillna(\"\", inplace=True)\n",
    "        for lang_feature in self.feature_names_:\n",
    "            X[lang_feature] = [(1 if lang_feature == \"lang_\"+v else 0) for v in X[\"lang\"].values]\n",
    "        \n",
    "        X.drop([\"lang\"], axis=1, inplace=True)\n",
    "        return X\n",
    "    \n",
    "lang_ohe = LangOneHotEncoding().fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special characters Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# class SpecialCharactersTransformer(TransformerMixin):\n",
    "\n",
    "#     def __init__(self, text_fields):\n",
    "#         self.text_fields = text_fields\n",
    "\n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def treat_special_char(self, c):\n",
    "#         try:\n",
    "#             encoding = chardet.detect(str(c))['encoding'] or \"KOI8-R\"\n",
    "#             return '0' if c.isdigit() else c.decode(encoding)\n",
    "#         except UnicodeDecodeError:        \n",
    "#             return '9'\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         for field in self.text_fields:\n",
    "# #             X.ix[X[field].isnull(), field] = \"null\"\n",
    "# #             X[field] = map(lambda n: ''.join(map(lambda c: self.treat_special_char(c), list(n))), X[field].values)\n",
    "#             X[field].fillna(\"null\", inplace=True)\n",
    "#             X[field] = [''.join([self.treat_special_char(c) for c in list(n)]) for n in X[field].values]\n",
    "\n",
    "#         return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameTfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def __init__(self, col, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "        super(DataFrameTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self.col = col\n",
    "        \n",
    "    def treat_special_char(self, c):\n",
    "        try:\n",
    "            encoding = chardet.detect(str(c))['encoding'] or \"KOI8-R\"\n",
    "            return '0' if c.isdigit() else c.decode(encoding)\n",
    "        except:        \n",
    "            return '9'\n",
    "\n",
    "    def treat_special_chars(self, col):\n",
    "        col.fillna(\"null\", inplace=True)\n",
    "        col = [''.join([self.treat_special_char(c) for c in list(n)]) \n",
    "               for n in col.values]\n",
    "        return col\n",
    "\n",
    "    def fit(self, dataframe, y=None):\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        super(DataFrameTfidfVectorizer, self).fit(dataframe[self.col])\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, dataframe, y=None):\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        field_matrix = super(DataFrameTfidfVectorizer, self).fit_transform(dataframe[self.col])\n",
    "        features_names = map(lambda f: \"_\".join([self.col,f]), super(DataFrameTfidfVectorizer, self).get_feature_names())\n",
    "        field_df = pd.DataFrame(field_matrix.A, columns=features_names)\n",
    "\n",
    "        dataframe = pd.concat([dataframe, field_df], axis=1, join='inner')\n",
    "        dataframe.drop([self.col], axis=1, inplace=True)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def transform(self, dataframe, copy=True):\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        field_matrix = super(DataFrameTfidfVectorizer, self).transform(dataframe[self.col])\n",
    "        features_names = map(lambda f: \"_\".join([self.col,f]), super(DataFrameTfidfVectorizer, self).get_feature_names())\n",
    "        field_df = pd.DataFrame(field_matrix.A, columns=features_names)\n",
    "\n",
    "        dataframe = pd.concat([dataframe, field_df], axis=1, join='inner')\n",
    "        dataframe.drop([self.col], axis=1, inplace=True)\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameTfidfVectorizer for textual fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_tfidf = DataFrameTfidfVectorizer(col=\"name\", \n",
    "                                      ngram_range=(3, 5), \n",
    "                                      analyzer=\"char\",\n",
    "                                      binary=True, #False\n",
    "                                      min_df = 50) #8\n",
    "\n",
    "screen_name_tfidf = DataFrameTfidfVectorizer(col=\"screen_name\", \n",
    "                                             ngram_range=(3, 5), \n",
    "                                             analyzer=\"char\",\n",
    "                                             binary=True, #False\n",
    "                                             min_df = 50) #8\n",
    "\n",
    "summary_tfidf = DataFrameTfidfVectorizer(col=\"summary\",\n",
    "                                         token_pattern=r'\\w+',\n",
    "                                         ngram_range=(1, 3), \n",
    "                                         analyzer=\"word\",\n",
    "                                         binary=True, #False\n",
    "                                         sublinear_tf=True, \n",
    "                                         stop_words='english',\n",
    "                                         min_df = 50) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrameTfidfVectorizer(analyzer='word', binary=True, col='summary',\n",
       "             decode_error='strict', dtype=<type 'numpy.int64'>,\n",
       "             encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "             max_features=None, min_df=50, ngram_range=(1, 3), norm=u'l2',\n",
       "             preprocessor=None, smooth_idf=True, stop_words='english',\n",
       "             strip_accents=None, sublinear_tf=False, token_pattern='\\\\w+',\n",
       "             tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_tfidf.fit(train)\n",
    "screen_name_tfidf.fit(train)\n",
    "summary_tfidf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X.drop([c], axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Imputer and np array transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumpyArrayTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X.fillna(0, inplace=True)\n",
    "        return np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist Enriched Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train.to_csv('data/manually_categorized/enriched-actor_classification_train.csv', index=False, encoding=\"utf-8\")\n",
    "# joblib.dump(train.columns, 'data/manually_categorized/actor_classification_random_forest_features_20151124.csv', compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome = \"manual_segment\"\n",
    "\n",
    "features = list(set(train.columns) - set([outcome]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## KFold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/sklearn/cross_validation.py:69: DeprecationWarning: The indices parameter is deprecated and will be removed (assumed True) in 0.17\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels=15183 does not match number of samples=6403",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-ea3ad155848c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf__n_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    139\u001b[0m         \"\"\"\n\u001b[1;32m    140\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pre_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[0;32m--> 273\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/ensemble/forest.pyc\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_counts\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/tree/tree.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[0;32m--> 221\u001b[0;31m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"min_samples_split must be greater than zero.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Number of labels=15183 does not match number of samples=6403"
     ]
    }
   ],
   "source": [
    "# KFold cross validation setup\n",
    "k_fold = KFold(n=len(train), n_folds=4, indices=False, shuffle=True)\n",
    "b_scores, svc_scores = [], []\n",
    "\n",
    "n_estimators = 10\n",
    "\n",
    "for tr_indices, cv_indices in k_fold:\n",
    "    tr   = train[tr_indices][features]\n",
    "    cv   = train[cv_indices][features]\n",
    "\n",
    "    tr_y = np.asarray(train[tr_indices][outcome])\n",
    "    cv_y = np.asarray(train[cv_indices][outcome])\n",
    "    \n",
    "    # Model Pipeline\n",
    "    model = Pipeline([(\"drop_cols\", DropColumnsTransformer([\"segment\",\"link\"])),\n",
    "                      (\"verified\", VerifiedTransformer()),\n",
    "                      (\"lang\", lang_ohe),\n",
    "                      (\"name_tfidf\", name_tfidf),\n",
    "                      (\"screen_name_tfidf\", screen_name_tfidf),\n",
    "                      (\"summary_tfidf\", summary_tfidf),\n",
    "                      (\"nparray\", NumpyArrayTransformer()),\n",
    "                      (\"scaler\", StandardScaler()),\n",
    "                      (\"rf\", RandomForestClassifier())])\n",
    "    model.set_params(rf__n_estimators = n_estimators)\n",
    "\n",
    "    model.fit(tr, tr_y)\n",
    "\n",
    "    # Validate\n",
    "    cv = model.transform(cv)\n",
    "    print(confusion_matrix(cv_y, model.predict(cv)))    \n",
    "    print('score:' + str(model.score(cv, cv_y)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Pipeline([(\"drop_cols\", DropColumnsTransformer([\"segment\",\"link\"])),\n",
    "                  (\"verified\", VerifiedTransformer()),\n",
    "                  (\"lang\", lang_ohe),\n",
    "                  (\"name_tfidf\", name_tfidf),\n",
    "                  (\"screen_name_tfidf\", screen_name_tfidf),\n",
    "                  (\"summary_tfidf\", summary_tfidf),\n",
    "                  (\"nparray\", NumpyArrayTransformer()),\n",
    "                  (\"scaler\", StandardScaler()),\n",
    "                  (\"rf\", RandomForestClassifier())])\n",
    "model.set_params(rf__n_estimators = n_estimators)\n",
    "model.fit(train[features], train[outcome])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = \"data/manually_categorized/actor_classification_random_forest_20151129.pkl\"\n",
    "joblib.dump(model, model_path, compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model           = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['{\"name\":\"Светлана Петухова\",\"screen_name\":\"svpetuhova26623\",\"summary\":\"\",\"lang\":\"ru\",\"favourites_count\":23,\"statuses_count\":14,\"friends_count\":2,\"followers_count\":1,\"listed_count\":1,\"verified\":0}',\n",
    "             '{\"lang\":\"en\",\"summary\":\"Artist, Writer, Designer. Tweets on tech, culture, art, animals, love the socioeconomy.\",\"verified\":0,\"followers_count\":175,\"friends_count\":397,\"favourites_count\":228,\"statuses_count\":410,\"listed_count\":12,\"name\":\"Daniel Adornes\",\"screen_name\":\"daniel_adornes\"}',\n",
    "             '{\"lang\":\"en\",\"summary\":\"Artist, Writer, Designer. Tweets on tech, culture, art, animals, love the socioeconomy.\",\"verified\":0,\"followers_count\":175,\"friends_count\":397,\"favourites_count\":228,\"statuses_count\":410,\"listed_count\":12,\"name\":\"Daniel Adornes\",\"screen_name\":\"daniel_adornes\"}']\n",
    "test_data = [json.loads(t) for t in test_data]\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = test_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(result, columns=[\"business\",\"person\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
