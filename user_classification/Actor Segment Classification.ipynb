{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fnmatch\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chardet\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.decomposition import PCA, RandomizedPCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_dir = 'data/manually_categorized/'\n",
    "input_prefix = 'actor_classification_train'\n",
    "train = None\n",
    "for file in os.listdir(input_dir):\n",
    "  if fnmatch.fnmatch(file, input_prefix+'*.csv'):\n",
    "    if train is None:\n",
    "      print \"==> Initializing input dataframe: \"\n",
    "      train = pd.read_csv(open(input_dir+file,'rU'),\n",
    "                          engine='python', sep=\",\", quoting=1)\n",
    "    else:\n",
    "      print \"==> Concatenating dataframe from \" + file + \": \"\n",
    "      train = pd.concat([train, pd.read_csv(open(input_dir+file,'rU'),\n",
    "                          engine='python', sep=\",\", quoting=1)])\n",
    "    train.drop_duplicates(inplace=True)\n",
    "    print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train = train.sample(1000)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer with LabelEncoder for 'verified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VerifiedTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X.verified.fillna(False, inplace=True)\n",
    "        X.verified = LabelEncoder().fit_transform(X.verified)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verified_transformer = VerifiedTransformer()\n",
    "verified_transformer.transform(train)\n",
    "\n",
    "print train.verified.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom OneHotEncoding for lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LangOneHotEncoding(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        valid_langs = list(set(X.lang) - set([None, np.nan, 'Select Language...']))\n",
    "        self.feature_names_ = [\"lang_\"+str(l) for l in valid_langs if type(l) == str]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self, 'feature_names_')\n",
    "        \n",
    "        X = X.copy()\n",
    "        X[\"lang\"].fillna(\"\", inplace=True)\n",
    "        for lang_feature in self.feature_names_:\n",
    "            X[lang_feature] = [(1 if lang_feature == \"lang_\"+v else 0) for v in X[\"lang\"].values]\n",
    "        \n",
    "        X.drop([\"lang\"], axis=1, inplace=True)\n",
    "        return X\n",
    "    \n",
    "lang_ohe = LangOneHotEncoding().fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing NAs on textual fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FillTextNA(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols, replace_by=\"\"):\n",
    "        self.cols = cols\n",
    "        self.replace_by = replace_by\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[c].fillna(self.replace_by, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameTfidfVectorizer for textual fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataFrameTfidfVectorizer(TfidfVectorizer):\n",
    "\n",
    "    def __init__(self, col, prefix=None, input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.int64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False):\n",
    "        super(DataFrameTfidfVectorizer, self).__init__(\n",
    "            input=input, encoding=encoding, decode_error=decode_error,\n",
    "            strip_accents=strip_accents, lowercase=lowercase,\n",
    "            preprocessor=preprocessor, tokenizer=tokenizer, analyzer=analyzer,\n",
    "            stop_words=stop_words, token_pattern=token_pattern,\n",
    "            ngram_range=ngram_range, max_df=max_df, min_df=min_df,\n",
    "            max_features=max_features, vocabulary=vocabulary, binary=binary,\n",
    "            dtype=dtype)\n",
    "\n",
    "        self.col = col\n",
    "        self.prefix = prefix or col\n",
    "        \n",
    "    def treat_special_char(self, c):\n",
    "        try:\n",
    "            encoding = chardet.detect(str(c))['encoding'] or \"KOI8-R\"\n",
    "            return '0' if c.isdigit() else c.decode(encoding)\n",
    "        except:        \n",
    "            return '9'\n",
    "\n",
    "    def treat_special_chars(self, col):\n",
    "        col.fillna(\"null\", inplace=True)\n",
    "        col = [''.join([self.treat_special_char(c) for c in list(n)]) \n",
    "               for n in col.values]\n",
    "        return col\n",
    "\n",
    "    def fit(self, dataframe, y=None):\n",
    "        dataframe = dataframe.copy()\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        super(DataFrameTfidfVectorizer, self).fit(dataframe[self.col])\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, dataframe, y=None):\n",
    "        dataframe = dataframe.copy()\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        field_matrix = super(DataFrameTfidfVectorizer, self).fit_transform(dataframe[self.col])\n",
    "        features_names = map(lambda f: \"_\".join([self.prefix,f]), super(DataFrameTfidfVectorizer, self).get_feature_names())\n",
    "        field_df = pd.DataFrame(field_matrix.A, columns=features_names)\n",
    "\n",
    "        dataframe = dataframe.join(field_df)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def transform(self, dataframe, copy=True):\n",
    "        dataframe = dataframe.copy()\n",
    "        dataframe[self.col] = self.treat_special_chars(dataframe[self.col])\n",
    "        field_matrix = super(DataFrameTfidfVectorizer, self).transform(dataframe[self.col])\n",
    "        features_names = map(lambda f: \"_\".join([self.prefix,f]), super(DataFrameTfidfVectorizer, self).get_feature_names())\n",
    "        field_df = pd.DataFrame(field_matrix.A, columns=features_names)\n",
    "\n",
    "        dataframe = dataframe.join(field_df)\n",
    "\n",
    "        return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_chars_tfidf = DataFrameTfidfVectorizer(col=\"name\", \n",
    "                                            prefix=\"name_c\",\n",
    "                                            ngram_range=(3, 5), \n",
    "                                            analyzer=\"char\",\n",
    "                                            binary=True, #False\n",
    "                                            min_df = 50) #8\n",
    "\n",
    "name_words_tfidf = DataFrameTfidfVectorizer(col=\"name\", \n",
    "                                            prefix=\"name_w\", \n",
    "                                            token_pattern=r'\\w+',\n",
    "                                            ngram_range=(1, 2), \n",
    "                                            analyzer=\"word\",\n",
    "                                            binary=True, #False\n",
    "                                            min_df = 10) #8\n",
    "\n",
    "screen_name_tfidf = DataFrameTfidfVectorizer(col=\"screen_name\", \n",
    "                                             ngram_range=(3, 5), \n",
    "                                             analyzer=\"char\",\n",
    "                                             binary=True, #False\n",
    "                                             min_df = 50) #8\n",
    "\n",
    "summary_tfidf = DataFrameTfidfVectorizer(col=\"summary\",\n",
    "                                         token_pattern=r'\\w+',\n",
    "                                         ngram_range=(1, 3), \n",
    "                                         analyzer=\"word\",\n",
    "                                         binary=True, #False\n",
    "                                         sublinear_tf=True, \n",
    "                                         stop_words='english',\n",
    "                                         min_df = 50) #5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# name_chars_tfidf.fit(train)\n",
    "# name_words_tfidf.fit(train)\n",
    "# screen_name_tfidf.fit(train)\n",
    "# summary_tfidf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further textual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextToLowerCase(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[c] = [t.lower() for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberOfWords(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_words_in_\"+c] = [len(t.split(' ')) for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberNonAlphaNumChars(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_non_alphanum_in_\"+c] = [len(re.sub(r\"[\\w\\d]\",\"\", t)) for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberUpperCaseChars(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_upper_case_chars_in_\"+c] = [len(re.sub(r\"[^A-Z]\",\"\", t)) for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumberCamelCaseWords(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_camel_case_words_in_\"+c] = [len(re.findall(r\"^[A-Z][a-z]|\\s[A-Z][a-z]\", t)) \n",
    "                                                         for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumberOfMentions(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_mentions_in_\"+c] = [len(re.findall(r\"\\s@[a-zA-Z]\",t)) \n",
    "                                                         for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumberOfPeriods(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"number_of_periods_in_\"+c] = [len(t.split(\". \")) \n",
    "                                                         for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AvgWordsPerPeriod(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"avg_words_per_period_in_\"+c] = [np.mean([len(p.split(\" \")) for p in t.split(\". \")]) \n",
    "                                                         for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MentionToFamilyRelation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def count_mentions(self, t):\n",
    "        count = 0\n",
    "        for o in [\"husband\",\"wife\",\"father\",\"mother\",\"daddy\",\"mommy\",\n",
    "                  \"grandfather\",\"grandmother\",\"grandpa\",\"grandma\"]:\n",
    "            count += len(re.findall(r\"(^|\\W)%s(\\W|$)\" % o, t))\n",
    "        return count\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"mention_to_family_relation_in_\"+c] = [self.count_mentions(t) \n",
    "                                                         for t in X[c].values]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professional Occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions to US occupations according to http://data.okfn.org/data/johnlsheridan/occupations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "occupations = pd.read_csv(\"https://raw.githubusercontent.com/johnlsheridan/occupations/master/occupations.csv\")\n",
    "occupations.Occupations = [o.lower() for o in occupations.Occupations.values]\n",
    "occupations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MentionToOccupation(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        occupations = pd.read_csv(\"https://raw.githubusercontent.com/johnlsheridan/occupations/master/occupations.csv\")\n",
    "        self.occupations_ = [o.lower() for o in occupations.Occupations.values]\n",
    "        return self\n",
    "    \n",
    "    def count_mentions(self, t):\n",
    "        count = 0\n",
    "        for o in self.occupations_:\n",
    "            count += len(re.findall(r\"(^|\\W)%s(\\W|$)\" % o, t))\n",
    "        return count\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self, 'occupations_')\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"mention_to_occupation_in_\"+c] = [self.count_mentions(t) \n",
    "                                                     for t in X[c].values]\n",
    "        return X\n",
    "    \n",
    "mention_to_occupation = MentionToOccupation([\"summary\"]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Person Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person names according to http://deron.meranda.us/data/census-dist-female-first.txt and http://deron.meranda.us/data/census-dist-male-first.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_names = pd.read_csv(\"http://deron.meranda.us/data/census-dist-female-first.txt\", names=[\"name\"])\n",
    "male_names   = pd.read_csv(\"http://deron.meranda.us/data/census-dist-male-first.txt\", names=[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "female_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "male_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "female_names = [re.sub(r\"[^a-z]\",\"\",n.lower()) for n in female_names.name.values]\n",
    "male_names   = [re.sub(r\"[^a-z]\",\"\",n.lower()) for n in male_names.name.values]\n",
    "person_names = list(set(male_names + female_names))\n",
    "print len(female_names)\n",
    "print len(male_names)\n",
    "print len(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PersonNames(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        female_names = pd.read_csv(\"http://deron.meranda.us/data/census-dist-female-first.txt\", names=[\"name\"])\n",
    "        male_names   = pd.read_csv(\"http://deron.meranda.us/data/census-dist-male-first.txt\", names=[\"name\"])\n",
    "        female_names = [re.sub(r\"[^a-z]\",\"\",n.lower()) for n in female_names.name.values]\n",
    "        male_names   = [re.sub(r\"[^a-z]\",\"\",n.lower()) for n in male_names.name.values]        \n",
    "        self.person_names_ = list(set(male_names + female_names))\n",
    "        return self\n",
    "    \n",
    "    def count_mentions(self, t):\n",
    "        count = 0\n",
    "        for n in self.person_names_:\n",
    "            count += len(re.findall(r\"(^|\\W)%s(\\W|$)\" % n, t))\n",
    "        return count\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        check_is_fitted(self, 'person_names_')\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X[\"person_names_in_\"+c] = [self.count_mentions(t) \n",
    "                                            for t in X[c].values]\n",
    "        return X\n",
    "    \n",
    "person_names = PersonNames([\"name\"]).fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Columns Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DropColumnsTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c in X:\n",
    "                X.drop([c], axis=1, inplace=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Imputer and np array transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NumpyArrayTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        X = X.reindex_axis(sorted(X.columns), axis=1)\n",
    "        X.fillna(0, inplace=True)\n",
    "        return np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Debugger for pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Debugger(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, name=\"\"):\n",
    "        self.name = name\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print \"-------------------------\"\n",
    "        print type(X)\n",
    "        print \"-------------------------\"\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outcome = \"manual_segment\"\n",
    "\n",
    "features = list(set(train.columns) - set([outcome]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier - 0.69\n",
    "# LogisticRegression - 0.57\n",
    "# GaussianNB 0.45\n",
    "# LinearSVC 0.5\n",
    "# KNeighborsClassifier 0.58\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "# Model Pipeline\n",
    "pipeline = Pipeline([ (\"drop_cols\", DropColumnsTransformer([\"segment\",\"link\"])),\n",
    "                      (\"verified\", VerifiedTransformer()),\n",
    "                      (\"lang\", lang_ohe),\n",
    "                      (\"fill_text_na\", FillTextNA([\"screen_name\",\"name\",\"summary\"], \"null\")),\n",
    "                      (\"qt_words\", NumberOfWords([\"name\",\"summary\"])),\n",
    "                      (\"qt_non_alphanum_chars\", NumberNonAlphaNumChars([\"name\",\"summary\"])),\n",
    "                      (\"qt_upper_case_chars\", NumberUpperCaseChars([\"name\",\"summary\"])),\n",
    "                      (\"qt_camel_case_words\", NumberCamelCaseWords([\"name\",\"summary\"])),\n",
    "                      (\"qt_mentions\", NumberOfMentions([\"summary\"])),\n",
    "                      (\"qt_periods\", NumberOfPeriods([\"summary\"])),\n",
    "                      (\"avg_words_per_period\", AvgWordsPerPeriod([\"summary\"])),\n",
    "                      (\"lower_case\", TextToLowerCase([\"screen_name\",\"name\",\"summary\"])),\n",
    "                      (\"family\", MentionToFamilyRelation([\"summary\"])),\n",
    "                      (\"person_names\", person_names),\n",
    "                      (\"occupations\", mention_to_occupation),\n",
    "                      (\"name_chars_tfidf\", name_chars_tfidf),\n",
    "                      (\"name_words_tfidf\", name_words_tfidf),\n",
    "                      (\"screen_name_tfidf\", screen_name_tfidf),\n",
    "                      (\"summary_tfidf\", summary_tfidf),\n",
    "                      (\"drop_text_cols\", DropColumnsTransformer([\"screen_name\",\"name\",\"summary\"])),\n",
    "                      (\"nparray\", NumpyArrayTransformer()),\n",
    "                      (\"model\", RandomForestClassifier())])\n",
    "\n",
    "# GridSearchCV params\n",
    "parameters = [\n",
    "    {\"name_chars_tfidf__analyzer\": [\"char\", \"char_wb\"]},\n",
    "    {\"name_chars_tfidf__min_df\": [10, 30, 50, 80]},\n",
    "    {\"name_words_tfidf__min_df\": [10, 30, 50, 80]},\n",
    "    {\"screen_name_tfidf__min_df\": [10, 30, 50, 80]},\n",
    "    {\"summary_tfidf__min_df\": [10, 30, 50, 80]},\n",
    "    {\"model__n_estimators\": [n_estimators]},\n",
    "]\n",
    "\n",
    "# GridSearchCV setup\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, error_score=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(train.loc[:,features], train.loc[:,outcome])\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param in parameters:\n",
    "    for param_name in param.keys():\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline.set_params(**best_parameters)\n",
    "pipeline.fit(train.loc[:,features], train.loc[:,outcome])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_path = \"data/manually_categorized/actor_classification_trained_model_20151129.pkl\"\n",
    "joblib.dump(pipeline, model_path, compress=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = ['{\"name\":\"Светлана Петухова\",\"screen_name\":\"svpetuhova26623\",\"summary\":\"\",\"lang\":\"ru\",\"favourites_count\":23,\"statuses_count\":14,\"friends_count\":2,\"followers_count\":1,\"listed_count\":1,\"verified\":0}',\n",
    "             '{\"lang\":\"en\",\"summary\":\"Artist, Writer, Designer. Tweets on tech, culture, art, animals, love the socioeconomy.\",\"verified\":0,\"followers_count\":175,\"friends_count\":397,\"favourites_count\":228,\"statuses_count\":410,\"listed_count\":12,\"name\":\"Daniel Adornes\",\"screen_name\":\"daniel_adornes\"}',\n",
    "             '{\"lang\":\"en\",\"summary\":\"Artist, Writer, Designer. Tweets on tech, culture, art, animals, love the socioeconomy.\",\"verified\":0,\"followers_count\":175,\"friends_count\":397,\"favourites_count\":228,\"statuses_count\":410,\"listed_count\":12,\"name\":\"Daniel Adornes\",\"screen_name\":\"daniel_adornes\"}']\n",
    "test_data = [json.loads(t) for t in test_data]\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = test_model.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(result, columns=[\"business\",\"person\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
