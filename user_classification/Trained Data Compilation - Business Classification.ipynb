{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loading Trained Data\n",
    "This data was trained by Encore Alert team in the business classification trainner on http://encorealert.com:8088\n",
    "\n",
    "## File Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Parsed data/trained/twitter-actor.csv with 135 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150606 with 2 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150607 with 309 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150608 with 59 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150609 with 3 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150610 with 1039 lines.\n",
      "# Parsed data/trained/twitter-actor.csv.20150612 with 468 lines.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "trained_files = glob.glob(\"data/trained/twitter-actor.csv*\")\n",
    "trained_dataframes = []\n",
    "for file in trained_files:\n",
    "    df = pd.read_csv(file, header=None, names=['native_id', 'class'])\n",
    "    trained_dataframes.append(df)\n",
    "    print('# Parsed', file, 'with', len(df), 'lines.')\n",
    "trained_data = pd.concat(trained_dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 427078784064294912</td>\n",
       "      <td> personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 427115022842867713</td>\n",
       "      <td> business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 427244265346195456</td>\n",
       "      <td> business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 427334246957473792</td>\n",
       "      <td> personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 427353217144336384</td>\n",
       "      <td> personal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            native_id     class\n",
       "0  427078784064294912  personal\n",
       "1  427115022842867713  business\n",
       "2  427244265346195456  business\n",
       "3  427334246957473792  personal\n",
       "4  427353217144336384  personal"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bot</th>\n",
       "      <td>   42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>  874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal</th>\n",
       "      <td> 1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>   62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          native_id\n",
       "class              \n",
       "bot              42\n",
       "business        874\n",
       "personal       1037\n",
       "spam             62"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>native_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td> 603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>personal</th>\n",
       "      <td> 751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          native_id\n",
       "class              \n",
       "business        603\n",
       "personal        751"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Filter only the classes that we have minimal amount of data\n",
    "# In this example, there is not enough bots, so ignoring them from now\n",
    "\n",
    "trained_data = trained_data[trained_data['class'].isin(['business', 'personal'])]\n",
    "trained_data = trained_data.drop_duplicates('native_id')\n",
    "trained_data['native_id'] = trained_data['native_id'].astype('str')\n",
    "trained_data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Twitter to retrieve metadata\n",
    "The scraped data consist mainly of handles and categories, so we need to retrieve extra actor information for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from twitter import *\n",
    "\n",
    "token = '22911906-GR7LBJ2oil3cc27aUIAln4zur4F7CdKAKyEi6NDzi'\n",
    "token_key = 'FZbyPm1i3BMfiXKlKPuzBdRlvbenW09n8LX5OvgM85g'\n",
    "con_secret = 'cyZ6NLdySvTkhKGUGmXMKw'\n",
    "con_secret_key = '5UgOJOanohNPMVkfLY85CjzdMcNAAVBlRCyGYys'\n",
    "\n",
    "t = Twitter(auth=OAuth(token, token_key, con_secret, con_secret_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def lookup_twitter_statuses(data):\n",
    "    # Get trained_data buckets too lookup\n",
    "    indices = np.arange(len(data))\n",
    "    max_mod = int((len(data)/100)+1)\n",
    "    tweets = []\n",
    "    for x in range(0, max_mod):\n",
    "        native_ids = data[(indices % max_mod) == x]['native_id']\n",
    "        tweets += t.statuses.lookup(_id=','.join(native_ids), _timeout=3)\n",
    "        print('Downloaded tweet info for', len(tweets))\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from html import escape\n",
    "\n",
    "def persist_complete_data(file_name, data):\n",
    "    \n",
    "    s = 0\n",
    "    with open(file_name, 'w') as csv_file:\n",
    "        tweets_writer = csv.writer(csv_file)\n",
    "        tweets_writer.writerow([\n",
    "            'actor_id',\n",
    "            'actor_screen_name',\n",
    "            'actor_name',\n",
    "            'actor_verified',\n",
    "            'actor_friends_count',\n",
    "            'actor_followers_count',\n",
    "            'actor_listed_count',\n",
    "            'actor_statuses_count',\n",
    "            'actor_favorites_count',\n",
    "            'actor_summary',\n",
    "            'actor_created_at',\n",
    "            'actor_location',\n",
    "\n",
    "            'tweet_id',\n",
    "            'tweet_created_at',\n",
    "            'tweet_generator',\n",
    "            'tweet_body',\n",
    "            'tweet_verb',\n",
    "\n",
    "            'tweet_urls_count',\n",
    "            'tweet_mentions_count',\n",
    "            'tweet_hashtags_count',\n",
    "            'tweet_trends_count',\n",
    "            'tweet_symbols_count'])\n",
    "        for tweet in data:\n",
    "            tweets_writer.writerow([\n",
    "                tweet['user']['id'],\n",
    "                tweet['user']['screen_name'],\n",
    "                tweet['user']['name'],\n",
    "                tweet['user']['verified'],\n",
    "                tweet['user']['friends_count'],\n",
    "                tweet['user']['followers_count'],\n",
    "                tweet['user']['listed_count'],\n",
    "                tweet['user']['statuses_count'],\n",
    "                tweet['user']['favourites_count'],\n",
    "                tweet['user']['description'],\n",
    "                tweet['user']['created_at'],\n",
    "                tweet['user']['location'] if tweet['user'].get('location') else 'null',\n",
    "\n",
    "                tweet['id'],\n",
    "                tweet['created_at'],\n",
    "                re.findall('>(.*)<', tweet['source'])[0],\n",
    "                tweet['text'],\n",
    "                not tweet['retweeted'],\n",
    "                len(tweet['entities']['urls']),\n",
    "                len(tweet['entities']['user_mentions']),\n",
    "                len(tweet['entities']['hashtags']),\n",
    "                \"\",\n",
    "                len(tweet['entities']['symbols'])\n",
    "            ])\n",
    "            s += 1\n",
    "            if (s % 100 == 0): print('Already writed', s, 'rows to', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded tweet info for 87\n",
      "Downloaded tweet info for 173\n",
      "Downloaded tweet info for 258\n",
      "Downloaded tweet info for 341\n",
      "Downloaded tweet info for 426\n",
      "Downloaded tweet info for 512\n",
      "Downloaded tweet info for 597\n",
      "Already writed 100 rows to data/csv/manually_trained_business.csv\n",
      "Already writed 200 rows to data/csv/manually_trained_business.csv\n",
      "Already writed 300 rows to data/csv/manually_trained_business.csv\n",
      "Already writed 400 rows to data/csv/manually_trained_business.csv\n",
      "Already writed 500 rows to data/csv/manually_trained_business.csv\n"
     ]
    }
   ],
   "source": [
    "business_data = lookup_twitter_statuses(trained_data[trained_data['class'] == 'business'])\n",
    "persist_complete_data('data/csv/brand_manually_trained.csv', business_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already writed 100 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 200 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 300 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 400 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 500 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 600 rows to data/csv/manually_trained_personal.csv\n",
      "Already writed 700 rows to data/csv/manually_trained_personal.csv\n"
     ]
    }
   ],
   "source": [
    "personal_data = lookup_twitter_statuses(trained_data[trained_data['class'] == 'personal'])\n",
    "persist_complete_data('data/csv/person_manually_trained.csv', personal_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
